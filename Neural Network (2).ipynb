{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c7174cd-9e93-4fc8-87f8-1766f763849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ccbd0f86-4734-4402-85f3-f841f2e72f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "801f8d02-2d15-4eed-9381-610fde107a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_data = pd.read_csv(r'C:\\Users\\jeffr\\Downloads\\ptb_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ee14e7e8-4e68-4274-96e7-57e3eb556331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 111990 entries, 0 to 112026\n",
      "Data columns (total 82 columns):\n",
      " #   Column                                             Non-Null Count   Dtype  \n",
      "---  ------                                             --------------   -----  \n",
      " 0   DurationSecs                                       111990 non-null  float64\n",
      " 1   Half                                               111990 non-null  int64  \n",
      " 2   OppPossessionSecs                                  111990 non-null  float64\n",
      " 3   PositionId                                         111990 non-null  float64\n",
      " 4   PossessionSecs                                     111990 non-null  float64\n",
      " 5   Set Type                                           111990 non-null  int64  \n",
      " 6   Total Involved Tacklers                            111990 non-null  float64\n",
      " 7   Raw Tackle Number                                  111990 non-null  float64\n",
      " 8   RoundId                                            111990 non-null  int64  \n",
      " 9   RunOn                                              111990 non-null  int64  \n",
      " 10  Score                                              111990 non-null  float64\n",
      " 11  SeasonId                                           111990 non-null  int64  \n",
      " 12  SeqNumber                                          111990 non-null  int64  \n",
      " 13  Set                                                111990 non-null  int64  \n",
      " 14  ZonePossession                                     111990 non-null  int64  \n",
      " 15  GameTime                                           111990 non-null  float64\n",
      " 16  ElapsedTime                                        111990 non-null  float64\n",
      " 17  IsHome                                             111990 non-null  int64  \n",
      " 18  CurrentMargin                                      111990 non-null  float64\n",
      " 19  WeatherConditionName_Rain                          111990 non-null  int32  \n",
      " 20  WeatherConditionName_Showers                       111990 non-null  int32  \n",
      " 21  WeatherConditionName_Snow                          111990 non-null  int32  \n",
      " 22  WeatherConditionName_Unknown                       111990 non-null  int32  \n",
      " 23  Club Id_1d6cd83892ee4afdcd8ccd94f817b4a6           111990 non-null  int32  \n",
      " 24  Club Id_1d6cd83892ee4afdcd8ccd94f81ftnjhl3s        111990 non-null  int32  \n",
      " 25  Club Id_367ef61d2bc259e608027a8d349c933e           111990 non-null  int32  \n",
      " 26  Club Id_3b26834df063f9d51de216a07ec36929           111990 non-null  int32  \n",
      " 27  Club Id_58485e3acf60682c8fc37d9d521b3019           111990 non-null  int32  \n",
      " 28  Club Id_5e03a19f4d014a2220665cfd56522d35           111990 non-null  int32  \n",
      " 29  Club Id_837e03d56b4dba3b8a4a5425c0420abd           111990 non-null  int32  \n",
      " 30  Club Id_980c9c368ae4f1129ea0a6fdd711fa8f           111990 non-null  int32  \n",
      " 31  Club Id_a73752d38e4a78e3e14917f5435ffb6d           111990 non-null  int32  \n",
      " 32  Club Id_b53920c88e4eebf2faa9f4fb43b8944a           111990 non-null  int32  \n",
      " 33  Club Id_c03196722c1a837b39f79f1714db475d           111990 non-null  int32  \n",
      " 34  Club Id_c14e0139ad91a9741a5731a596aa6549           111990 non-null  int32  \n",
      " 35  Club Id_d3ac47d424b41fd738ec9500dbda2d59           111990 non-null  int32  \n",
      " 36  Club Id_dc3c7bd8148814b7c4105841baa68e23           111990 non-null  int32  \n",
      " 37  Club Id_f38f7f087f646c38c0207f1b2af32f12           111990 non-null  int32  \n",
      " 38  Club Id_fdfcde48e2cbf12cc4710a2644b86d85           111990 non-null  int32  \n",
      " 39  Opposition Id_1d6cd83892ee4afdcd8ccd94f817b4a6     111990 non-null  int32  \n",
      " 40  Opposition Id_1d6cd83892ee4afdcd8ccd94f81ftnjhl3s  111990 non-null  int32  \n",
      " 41  Opposition Id_367ef61d2bc259e608027a8d349c933e     111990 non-null  int32  \n",
      " 42  Opposition Id_3b26834df063f9d51de216a07ec36929     111990 non-null  int32  \n",
      " 43  Opposition Id_58485e3acf60682c8fc37d9d521b3019     111990 non-null  int32  \n",
      " 44  Opposition Id_5e03a19f4d014a2220665cfd56522d35     111990 non-null  int32  \n",
      " 45  Opposition Id_837e03d56b4dba3b8a4a5425c0420abd     111990 non-null  int32  \n",
      " 46  Opposition Id_980c9c368ae4f1129ea0a6fdd711fa8f     111990 non-null  int32  \n",
      " 47  Opposition Id_a73752d38e4a78e3e14917f5435ffb6d     111990 non-null  int32  \n",
      " 48  Opposition Id_b53920c88e4eebf2faa9f4fb43b8944a     111990 non-null  int32  \n",
      " 49  Opposition Id_c03196722c1a837b39f79f1714db475d     111990 non-null  int32  \n",
      " 50  Opposition Id_c14e0139ad91a9741a5731a596aa6549     111990 non-null  int32  \n",
      " 51  Opposition Id_d3ac47d424b41fd738ec9500dbda2d59     111990 non-null  int32  \n",
      " 52  Opposition Id_dc3c7bd8148814b7c4105841baa68e23     111990 non-null  int32  \n",
      " 53  Opposition Id_f38f7f087f646c38c0207f1b2af32f12     111990 non-null  int32  \n",
      " 54  Opposition Id_fdfcde48e2cbf12cc4710a2644b86d85     111990 non-null  int32  \n",
      " 55  PTB Contest_Other                                  111990 non-null  int32  \n",
      " 56  PTB Contest_Stays on feet                          111990 non-null  int32  \n",
      " 57  PTB Contest_Tackled to ground                      111990 non-null  int32  \n",
      " 58  PTB Ultimate Outcome_Field goal attempt            111990 non-null  int32  \n",
      " 59  PTB Ultimate Outcome_Handover                      111990 non-null  int32  \n",
      " 60  PTB Ultimate Outcome_Kick                          111990 non-null  int32  \n",
      " 61  PTB Ultimate Outcome_Opp ruck infringement         111990 non-null  int32  \n",
      " 62  PTB Ultimate Outcome_Other                         111990 non-null  int32  \n",
      " 63  PTB Ultimate Outcome_Penalty attack                111990 non-null  int32  \n",
      " 64  PTB Ultimate Outcome_Penalty defence               111990 non-null  int32  \n",
      " 65  PTB Ultimate Outcome_Try                           111990 non-null  int32  \n",
      " 66  PTB Ultimate Outcome_Turnover                      111990 non-null  int32  \n",
      " 67  OfficialId_500002                                  111990 non-null  int32  \n",
      " 68  OfficialId_500004                                  111990 non-null  int32  \n",
      " 69  OfficialId_500006                                  111990 non-null  int32  \n",
      " 70  OfficialId_500016                                  111990 non-null  int32  \n",
      " 71  OfficialId_500019                                  111990 non-null  int32  \n",
      " 72  OfficialId_500029                                  111990 non-null  int32  \n",
      " 73  OfficialId_500039                                  111990 non-null  int32  \n",
      " 74  OfficialId_500042                                  111990 non-null  int32  \n",
      " 75  OfficialId_500045                                  111990 non-null  int32  \n",
      " 76  OfficialId_500050                                  111990 non-null  int32  \n",
      " 77  OfficialId_500067                                  111990 non-null  int32  \n",
      " 78  OfficialId_500070                                  111990 non-null  int32  \n",
      " 79  OfficialId_500072                                  111990 non-null  int32  \n",
      " 80  OfficialId_500075                                  111990 non-null  int32  \n",
      " 81  OfficialId_500117                                  111990 non-null  int32  \n",
      "dtypes: float64(10), int32(63), int64(9)\n",
      "memory usage: 44.0 MB\n"
     ]
    }
   ],
   "source": [
    "# df_encoded = ptb_data.drop(columns = ['PTB Defence','Anonymize 1PlayerId', 'EventName', 'Club Id', 'Opposition Id', 'PTB Contest', \n",
    "#                                         'PTB Ultimate Outcome', 'WeatherConditionName'])\n",
    "\n",
    "\n",
    "df_encoded = ptb_data.drop(columns = ['PTB Defence', 'Anonymize 1PlayerId', 'Player Id', 'ZonePhysical', 'MatchId', 'Tackle', 'OppScore', 'Home Score', 'Away Score', 'PTB Tackle Result', 'EventName', 'TotalPossessionSecs'])\n",
    "\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['WeatherConditionName', 'Club Id', 'Opposition Id', 'PTB Contest', 'PTB Ultimate Outcome', 'OfficialId'], drop_first=True)\n",
    "\n",
    "\n",
    "df_encoded = df_encoded.dropna()\n",
    "\n",
    "df_encoded[df_encoded.select_dtypes(include=['bool']).columns] = df_encoded.select_dtypes(include=['bool']).astype(int)\n",
    "\n",
    "df_encoded.info()\n",
    "\n",
    "df_encoded.to_csv('encoded_ptb_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ef807a8c-bc4a-4d8e-97f3-b30c7a12257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50654410-0e0d-4bf4-944c-c737003cd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6fcc34fd-4f44-4edc-a4ef-e09f0f107479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>DurationSecs</td>   <th>  R-squared:         </th>  <td>   0.005</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.005</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   288.2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 05 Nov 2024</td> <th>  Prob (F-statistic):</th>  <td>1.46e-125</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:05:43</td>     <th>  Log-Likelihood:    </th> <td>-1.6760e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>111990</td>      <th>  AIC:               </th>  <td>3.352e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>111987</td>      <th>  BIC:               </th>  <td>3.352e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>         <td>    3.4071</td> <td>    0.011</td> <td>  303.729</td> <td> 0.000</td> <td>    3.385</td> <td>    3.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Half</th>              <td>   -0.0240</td> <td>    0.012</td> <td>   -2.038</td> <td> 0.042</td> <td>   -0.047</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OppPossessionSecs</th> <td>    0.0002</td> <td> 1.23e-05</td> <td>   14.573</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13105.824</td> <th>  Durbin-Watson:     </th> <td>   1.826</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>47357.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.571</td>   <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.974</td>   <th>  Cond. No.          </th> <td>4.57e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.57e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &   DurationSecs   & \\textbf{  R-squared:         } &      0.005   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.005   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      288.2   \\\\\n",
       "\\textbf{Date:}             & Tue, 05 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.46e-125   \\\\\n",
       "\\textbf{Time:}             &     20:05:43     & \\textbf{  Log-Likelihood:    } & -1.6760e+05  \\\\\n",
       "\\textbf{No. Observations:} &      111990      & \\textbf{  AIC:               } &  3.352e+05   \\\\\n",
       "\\textbf{Df Residuals:}     &      111987      & \\textbf{  BIC:               } &  3.352e+05   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &              \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &              \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}         &       3.4071  &        0.011     &   303.729  &         0.000        &        3.385    &        3.429     \\\\\n",
       "\\textbf{Half}              &      -0.0240  &        0.012     &    -2.038  &         0.042        &       -0.047    &       -0.001     \\\\\n",
       "\\textbf{OppPossessionSecs} &       0.0002  &     1.23e-05     &    14.573  &         0.000        &        0.000    &        0.000     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 13105.824 & \\textbf{  Durbin-Watson:     } &     1.826  \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 47357.703  \\\\\n",
       "\\textbf{Skew:}          &    0.571  & \\textbf{  Prob(JB):          } &      0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &    5.974  & \\textbf{  Cond. No.          } &  4.57e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 4.57e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:           DurationSecs   R-squared:                       0.005\n",
       "Model:                            OLS   Adj. R-squared:                  0.005\n",
       "Method:                 Least Squares   F-statistic:                     288.2\n",
       "Date:                Tue, 05 Nov 2024   Prob (F-statistic):          1.46e-125\n",
       "Time:                        20:05:43   Log-Likelihood:            -1.6760e+05\n",
       "No. Observations:              111990   AIC:                         3.352e+05\n",
       "Df Residuals:                  111987   BIC:                         3.352e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "Intercept             3.4071      0.011    303.729      0.000       3.385       3.429\n",
       "Half                 -0.0240      0.012     -2.038      0.042      -0.047      -0.001\n",
       "OppPossessionSecs     0.0002   1.23e-05     14.573      0.000       0.000       0.000\n",
       "==============================================================================\n",
       "Omnibus:                    13105.824   Durbin-Watson:                   1.826\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            47357.703\n",
       "Skew:                           0.571   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.974   Cond. No.                     4.57e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.57e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrl_train, nrl_test = train_test_split(df_encoded, test_size = 0.2,random_state=1)\n",
    "df_encoded.columns = df_encoded.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "\n",
    "formula = 'DurationSecs ~ ' + ' + '.join(df_encoded.columns.difference(['DurationSecs']))\n",
    "model = smf.ols(formula=formula, data=df_encoded).fit()\n",
    "predictions = model.predict(nrl_test)\n",
    "y_test = nrl_test.DurationSecs\n",
    "y_train = nrl_train.DurationSecs\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "51c05bd2-adab-4077-8640-143f4d9bd27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the test data for linear regression:  1.1740740381255725\n"
     ]
    }
   ],
   "source": [
    "MSE_lm = np.mean((predictions-y_test)**2)\n",
    "print('MSE on the test data for linear regression: ',MSE_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "5db66d73-4213-4a86-b260-94d6fe9f16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = nrl_train.drop('DurationSecs',axis='columns')\n",
    "scaler.fit(X_train) #fit scaler (estimate mean and std) on training data only!\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# apply the same transformation to test data\n",
    "X_test = nrl_test.drop('DurationSecs',axis='columns')\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "29995c16-c2fd-4f5f-b30c-e2f469b7d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.986388683257667e-17 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:,1].mean(),X_train[:,1].std()) #check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "4d1a2c58-8308-4654-9b53-70606423d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the test data for linear regression:  0.9547626863323189\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_1 = sm.add_constant(X_train)\n",
    "X_test_1 = sm.add_constant(X_test)\n",
    "model_sc = sm.OLS(y_train, X_train_1)\n",
    "lm_scaled = model_sc.fit()\n",
    "\n",
    "lm_scaled.summary()\n",
    "\n",
    "predictions_scaled = lm_scaled.predict(X_test_1)\n",
    "\n",
    "MSE_lm_scaled = np.mean((predictions_scaled-y_test)**2)\n",
    "print('MSE on the test data for linear regression: ',MSE_lm_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "6422b80e-d1d5-4b1b-a22d-fb2cfb2a48d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>DurationSecs</td>   <th>  R-squared:         </th>  <td>   0.187</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.186</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   254.5</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 05 Nov 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:18:24</td>     <th>  Log-Likelihood:    </th> <td>-1.2498e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 89592</td>      <th>  AIC:               </th>  <td>2.501e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 89510</td>      <th>  BIC:               </th>  <td>2.509e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    81</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    3.5155</td> <td>    0.003</td> <td> 1077.245</td> <td> 0.000</td> <td>    3.509</td> <td>    3.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.0171</td> <td>    0.033</td> <td>    0.515</td> <td> 0.606</td> <td>   -0.048</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.2253</td> <td>    0.033</td> <td>    6.837</td> <td> 0.000</td> <td>    0.161</td> <td>    0.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0502</td> <td>    0.004</td> <td>  -11.285</td> <td> 0.000</td> <td>   -0.059</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.2829</td> <td>    0.033</td> <td>    8.566</td> <td> 0.000</td> <td>    0.218</td> <td>    0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0880</td> <td>    0.006</td> <td>   15.198</td> <td> 0.000</td> <td>    0.077</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0346</td> <td>    0.003</td> <td>   10.131</td> <td> 0.000</td> <td>    0.028</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.1370</td> <td>    0.005</td> <td>  -28.244</td> <td> 0.000</td> <td>   -0.147</td> <td>   -0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.0148</td> <td>    0.004</td> <td>   -4.149</td> <td> 0.000</td> <td>   -0.022</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0171</td> <td>    0.004</td> <td>    4.058</td> <td> 0.000</td> <td>    0.009</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.0351</td> <td>    0.010</td> <td>    3.369</td> <td> 0.001</td> <td>    0.015</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.0629</td> <td>    0.004</td> <td>   14.713</td> <td> 0.000</td> <td>    0.055</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   -0.4380</td> <td>    0.060</td> <td>   -7.350</td> <td> 0.000</td> <td>   -0.555</td> <td>   -0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0239</td> <td>    0.030</td> <td>   -0.798</td> <td> 0.425</td> <td>   -0.083</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.0696</td> <td>    0.006</td> <td>   11.062</td> <td> 0.000</td> <td>    0.057</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    0.0158</td> <td>    0.032</td> <td>    0.500</td> <td> 0.617</td> <td>   -0.046</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.0258</td> <td>    0.027</td> <td>    0.970</td> <td> 0.332</td> <td>   -0.026</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>   -0.0191</td> <td>    0.003</td> <td>   -5.674</td> <td> 0.000</td> <td>   -0.026</td> <td>   -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.0211</td> <td>    0.008</td> <td>    2.612</td> <td> 0.009</td> <td>    0.005</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0098</td> <td>    0.003</td> <td>    2.840</td> <td> 0.005</td> <td>    0.003</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.0037</td> <td>    0.003</td> <td>   -1.083</td> <td> 0.279</td> <td>   -0.010</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -0.0019</td> <td>    0.003</td> <td>   -0.555</td> <td> 0.579</td> <td>   -0.009</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.0139</td> <td>    0.004</td> <td>    3.682</td> <td> 0.000</td> <td>    0.006</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.0044</td> <td>    0.005</td> <td>    0.907</td> <td> 0.365</td> <td>   -0.005</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>   -0.0092</td> <td>    0.004</td> <td>   -2.433</td> <td> 0.015</td> <td>   -0.017</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.0309</td> <td>    0.005</td> <td>   -6.074</td> <td> 0.000</td> <td>   -0.041</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>    0.0212</td> <td>    0.005</td> <td>    4.064</td> <td> 0.000</td> <td>    0.011</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    0.0157</td> <td>    0.005</td> <td>    3.305</td> <td> 0.001</td> <td>    0.006</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.0015</td> <td>    0.005</td> <td>   -0.307</td> <td> 0.759</td> <td>   -0.011</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.0067</td> <td>    0.005</td> <td>   -1.369</td> <td> 0.171</td> <td>   -0.016</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -0.0086</td> <td>    0.005</td> <td>   -1.748</td> <td> 0.080</td> <td>   -0.018</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>    0.0308</td> <td>    0.005</td> <td>    5.927</td> <td> 0.000</td> <td>    0.021</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>    0.0230</td> <td>    0.005</td> <td>    4.658</td> <td> 0.000</td> <td>    0.013</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>   -0.0214</td> <td>    0.005</td> <td>   -4.361</td> <td> 0.000</td> <td>   -0.031</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>    0.0215</td> <td>    0.005</td> <td>    4.378</td> <td> 0.000</td> <td>    0.012</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>   -0.0062</td> <td>    0.005</td> <td>   -1.221</td> <td> 0.222</td> <td>   -0.016</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.0161</td> <td>    0.005</td> <td>   -3.276</td> <td> 0.001</td> <td>   -0.026</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>   -0.0307</td> <td>    0.005</td> <td>   -6.114</td> <td> 0.000</td> <td>   -0.041</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>   -0.0090</td> <td>    0.005</td> <td>   -1.893</td> <td> 0.058</td> <td>   -0.018</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>    0.0010</td> <td>    0.005</td> <td>    0.207</td> <td> 0.836</td> <td>   -0.008</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>   -0.0427</td> <td>    0.004</td> <td>  -11.195</td> <td> 0.000</td> <td>   -0.050</td> <td>   -0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>   -0.0061</td> <td>    0.005</td> <td>   -1.189</td> <td> 0.235</td> <td>   -0.016</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>   -0.0268</td> <td>    0.005</td> <td>   -5.175</td> <td> 0.000</td> <td>   -0.037</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>   -0.0030</td> <td>    0.005</td> <td>   -0.627</td> <td> 0.531</td> <td>   -0.012</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>   -0.0166</td> <td>    0.005</td> <td>   -3.349</td> <td> 0.001</td> <td>   -0.026</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -0.0279</td> <td>    0.005</td> <td>   -5.681</td> <td> 0.000</td> <td>   -0.038</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   -0.0215</td> <td>    0.005</td> <td>   -4.341</td> <td> 0.000</td> <td>   -0.031</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>   -0.0269</td> <td>    0.005</td> <td>   -5.175</td> <td> 0.000</td> <td>   -0.037</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>   -0.0037</td> <td>    0.005</td> <td>   -0.741</td> <td> 0.459</td> <td>   -0.014</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>   -0.0310</td> <td>    0.005</td> <td>   -6.344</td> <td> 0.000</td> <td>   -0.041</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>    0.0106</td> <td>    0.005</td> <td>    2.185</td> <td> 0.029</td> <td>    0.001</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>   -0.0128</td> <td>    0.005</td> <td>   -2.550</td> <td> 0.011</td> <td>   -0.023</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>    0.0018</td> <td>    0.005</td> <td>    0.363</td> <td> 0.716</td> <td>   -0.008</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    0.0007</td> <td>    0.005</td> <td>    0.146</td> <td> 0.884</td> <td>   -0.009</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>   -0.0246</td> <td>    0.005</td> <td>   -5.113</td> <td> 0.000</td> <td>   -0.034</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>   -0.0038</td> <td>    0.003</td> <td>   -1.176</td> <td> 0.240</td> <td>   -0.010</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>   -0.2512</td> <td>    0.003</td> <td>  -73.767</td> <td> 0.000</td> <td>   -0.258</td> <td>   -0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>   -0.3631</td> <td>    0.003</td> <td> -105.698</td> <td> 0.000</td> <td>   -0.370</td> <td>   -0.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>   -0.0055</td> <td>    0.003</td> <td>   -1.685</td> <td> 0.092</td> <td>   -0.012</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>    0.0035</td> <td>    0.003</td> <td>    1.054</td> <td> 0.292</td> <td>   -0.003</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>    0.0179</td> <td>    0.004</td> <td>    4.590</td> <td> 0.000</td> <td>    0.010</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>    0.0026</td> <td>    0.003</td> <td>    0.791</td> <td> 0.429</td> <td>   -0.004</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>    0.0042</td> <td>    0.003</td> <td>    1.289</td> <td> 0.197</td> <td>   -0.002</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>    0.0074</td> <td>    0.003</td> <td>    2.243</td> <td> 0.025</td> <td>    0.001</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>   -0.0039</td> <td>    0.003</td> <td>   -1.189</td> <td> 0.234</td> <td>   -0.010</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>    0.0026</td> <td>    0.003</td> <td>    0.780</td> <td> 0.435</td> <td>   -0.004</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>   -0.0035</td> <td>    0.003</td> <td>   -1.043</td> <td> 0.297</td> <td>   -0.010</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>   -0.0077</td> <td>    0.004</td> <td>   -1.816</td> <td> 0.069</td> <td>   -0.016</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>   -0.0111</td> <td>    0.005</td> <td>   -2.172</td> <td> 0.030</td> <td>   -0.021</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>   -0.0122</td> <td>    0.004</td> <td>   -3.214</td> <td> 0.001</td> <td>   -0.020</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>   -0.0227</td> <td>    0.005</td> <td>   -4.689</td> <td> 0.000</td> <td>   -0.032</td> <td>   -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>   -0.0181</td> <td>    0.005</td> <td>   -3.578</td> <td> 0.000</td> <td>   -0.028</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>   -0.0197</td> <td>    0.004</td> <td>   -4.509</td> <td> 0.000</td> <td>   -0.028</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>   -0.0304</td> <td>    0.005</td> <td>   -5.595</td> <td> 0.000</td> <td>   -0.041</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>   -0.0177</td> <td>    0.004</td> <td>   -4.420</td> <td> 0.000</td> <td>   -0.025</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>   -0.0085</td> <td>    0.004</td> <td>   -2.420</td> <td> 0.016</td> <td>   -0.015</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>   -0.0034</td> <td>    0.005</td> <td>   -0.755</td> <td> 0.450</td> <td>   -0.012</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>   -0.0185</td> <td>    0.005</td> <td>   -3.659</td> <td> 0.000</td> <td>   -0.028</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>    0.0002</td> <td>    0.003</td> <td>    0.055</td> <td> 0.956</td> <td>   -0.007</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>   -0.0031</td> <td>    0.003</td> <td>   -0.900</td> <td> 0.368</td> <td>   -0.010</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>   -0.0095</td> <td>    0.004</td> <td>   -2.515</td> <td> 0.012</td> <td>   -0.017</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>   -0.0142</td> <td>    0.004</td> <td>   -3.241</td> <td> 0.001</td> <td>   -0.023</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>12172.876</td> <th>  Durbin-Watson:     </th> <td>   2.003</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>69343.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.528</td>   <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 7.179</td>   <th>  Cond. No.          </th> <td>    51.9</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &   DurationSecs   & \\textbf{  R-squared:         } &      0.187   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.186   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      254.5   \\\\\n",
       "\\textbf{Date:}             & Tue, 05 Nov 2024 & \\textbf{  Prob (F-statistic):} &      0.00    \\\\\n",
       "\\textbf{Time:}             &     20:18:24     & \\textbf{  Log-Likelihood:    } & -1.2498e+05  \\\\\n",
       "\\textbf{No. Observations:} &       89592      & \\textbf{  AIC:               } &  2.501e+05   \\\\\n",
       "\\textbf{Df Residuals:}     &       89510      & \\textbf{  BIC:               } &  2.509e+05   \\\\\n",
       "\\textbf{Df Model:}         &          81      & \\textbf{                     } &              \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &              \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       3.5155  &        0.003     &  1077.245  &         0.000        &        3.509    &        3.522     \\\\\n",
       "\\textbf{x1}    &       0.0171  &        0.033     &     0.515  &         0.606        &       -0.048    &        0.082     \\\\\n",
       "\\textbf{x2}    &       0.2253  &        0.033     &     6.837  &         0.000        &        0.161    &        0.290     \\\\\n",
       "\\textbf{x3}    &      -0.0502  &        0.004     &   -11.285  &         0.000        &       -0.059    &       -0.041     \\\\\n",
       "\\textbf{x4}    &       0.2829  &        0.033     &     8.566  &         0.000        &        0.218    &        0.348     \\\\\n",
       "\\textbf{x5}    &       0.0880  &        0.006     &    15.198  &         0.000        &        0.077    &        0.099     \\\\\n",
       "\\textbf{x6}    &       0.0346  &        0.003     &    10.131  &         0.000        &        0.028    &        0.041     \\\\\n",
       "\\textbf{x7}    &      -0.1370  &        0.005     &   -28.244  &         0.000        &       -0.147    &       -0.127     \\\\\n",
       "\\textbf{x8}    &      -0.0148  &        0.004     &    -4.149  &         0.000        &       -0.022    &       -0.008     \\\\\n",
       "\\textbf{x9}    &       0.0171  &        0.004     &     4.058  &         0.000        &        0.009    &        0.025     \\\\\n",
       "\\textbf{x10}   &       0.0351  &        0.010     &     3.369  &         0.001        &        0.015    &        0.056     \\\\\n",
       "\\textbf{x11}   &       0.0629  &        0.004     &    14.713  &         0.000        &        0.055    &        0.071     \\\\\n",
       "\\textbf{x12}   &      -0.4380  &        0.060     &    -7.350  &         0.000        &       -0.555    &       -0.321     \\\\\n",
       "\\textbf{x13}   &      -0.0239  &        0.030     &    -0.798  &         0.425        &       -0.083    &        0.035     \\\\\n",
       "\\textbf{x14}   &       0.0696  &        0.006     &    11.062  &         0.000        &        0.057    &        0.082     \\\\\n",
       "\\textbf{x15}   &       0.0158  &        0.032     &     0.500  &         0.617        &       -0.046    &        0.078     \\\\\n",
       "\\textbf{x16}   &       0.0258  &        0.027     &     0.970  &         0.332        &       -0.026    &        0.078     \\\\\n",
       "\\textbf{x17}   &      -0.0191  &        0.003     &    -5.674  &         0.000        &       -0.026    &       -0.013     \\\\\n",
       "\\textbf{x18}   &       0.0211  &        0.008     &     2.612  &         0.009        &        0.005    &        0.037     \\\\\n",
       "\\textbf{x19}   &       0.0098  &        0.003     &     2.840  &         0.005        &        0.003    &        0.017     \\\\\n",
       "\\textbf{x20}   &      -0.0037  &        0.003     &    -1.083  &         0.279        &       -0.010    &        0.003     \\\\\n",
       "\\textbf{x21}   &      -0.0019  &        0.003     &    -0.555  &         0.579        &       -0.009    &        0.005     \\\\\n",
       "\\textbf{x22}   &       0.0139  &        0.004     &     3.682  &         0.000        &        0.006    &        0.021     \\\\\n",
       "\\textbf{x23}   &       0.0044  &        0.005     &     0.907  &         0.365        &       -0.005    &        0.014     \\\\\n",
       "\\textbf{x24}   &      -0.0092  &        0.004     &    -2.433  &         0.015        &       -0.017    &       -0.002     \\\\\n",
       "\\textbf{x25}   &      -0.0309  &        0.005     &    -6.074  &         0.000        &       -0.041    &       -0.021     \\\\\n",
       "\\textbf{x26}   &       0.0212  &        0.005     &     4.064  &         0.000        &        0.011    &        0.031     \\\\\n",
       "\\textbf{x27}   &       0.0157  &        0.005     &     3.305  &         0.001        &        0.006    &        0.025     \\\\\n",
       "\\textbf{x28}   &      -0.0015  &        0.005     &    -0.307  &         0.759        &       -0.011    &        0.008     \\\\\n",
       "\\textbf{x29}   &      -0.0067  &        0.005     &    -1.369  &         0.171        &       -0.016    &        0.003     \\\\\n",
       "\\textbf{x30}   &      -0.0086  &        0.005     &    -1.748  &         0.080        &       -0.018    &        0.001     \\\\\n",
       "\\textbf{x31}   &       0.0308  &        0.005     &     5.927  &         0.000        &        0.021    &        0.041     \\\\\n",
       "\\textbf{x32}   &       0.0230  &        0.005     &     4.658  &         0.000        &        0.013    &        0.033     \\\\\n",
       "\\textbf{x33}   &      -0.0214  &        0.005     &    -4.361  &         0.000        &       -0.031    &       -0.012     \\\\\n",
       "\\textbf{x34}   &       0.0215  &        0.005     &     4.378  &         0.000        &        0.012    &        0.031     \\\\\n",
       "\\textbf{x35}   &      -0.0062  &        0.005     &    -1.221  &         0.222        &       -0.016    &        0.004     \\\\\n",
       "\\textbf{x36}   &      -0.0161  &        0.005     &    -3.276  &         0.001        &       -0.026    &       -0.006     \\\\\n",
       "\\textbf{x37}   &      -0.0307  &        0.005     &    -6.114  &         0.000        &       -0.041    &       -0.021     \\\\\n",
       "\\textbf{x38}   &      -0.0090  &        0.005     &    -1.893  &         0.058        &       -0.018    &        0.000     \\\\\n",
       "\\textbf{x39}   &       0.0010  &        0.005     &     0.207  &         0.836        &       -0.008    &        0.010     \\\\\n",
       "\\textbf{x40}   &      -0.0427  &        0.004     &   -11.195  &         0.000        &       -0.050    &       -0.035     \\\\\n",
       "\\textbf{x41}   &      -0.0061  &        0.005     &    -1.189  &         0.235        &       -0.016    &        0.004     \\\\\n",
       "\\textbf{x42}   &      -0.0268  &        0.005     &    -5.175  &         0.000        &       -0.037    &       -0.017     \\\\\n",
       "\\textbf{x43}   &      -0.0030  &        0.005     &    -0.627  &         0.531        &       -0.012    &        0.006     \\\\\n",
       "\\textbf{x44}   &      -0.0166  &        0.005     &    -3.349  &         0.001        &       -0.026    &       -0.007     \\\\\n",
       "\\textbf{x45}   &      -0.0279  &        0.005     &    -5.681  &         0.000        &       -0.038    &       -0.018     \\\\\n",
       "\\textbf{x46}   &      -0.0215  &        0.005     &    -4.341  &         0.000        &       -0.031    &       -0.012     \\\\\n",
       "\\textbf{x47}   &      -0.0269  &        0.005     &    -5.175  &         0.000        &       -0.037    &       -0.017     \\\\\n",
       "\\textbf{x48}   &      -0.0037  &        0.005     &    -0.741  &         0.459        &       -0.014    &        0.006     \\\\\n",
       "\\textbf{x49}   &      -0.0310  &        0.005     &    -6.344  &         0.000        &       -0.041    &       -0.021     \\\\\n",
       "\\textbf{x50}   &       0.0106  &        0.005     &     2.185  &         0.029        &        0.001    &        0.020     \\\\\n",
       "\\textbf{x51}   &      -0.0128  &        0.005     &    -2.550  &         0.011        &       -0.023    &       -0.003     \\\\\n",
       "\\textbf{x52}   &       0.0018  &        0.005     &     0.363  &         0.716        &       -0.008    &        0.012     \\\\\n",
       "\\textbf{x53}   &       0.0007  &        0.005     &     0.146  &         0.884        &       -0.009    &        0.011     \\\\\n",
       "\\textbf{x54}   &      -0.0246  &        0.005     &    -5.113  &         0.000        &       -0.034    &       -0.015     \\\\\n",
       "\\textbf{x55}   &      -0.0038  &        0.003     &    -1.176  &         0.240        &       -0.010    &        0.003     \\\\\n",
       "\\textbf{x56}   &      -0.2512  &        0.003     &   -73.767  &         0.000        &       -0.258    &       -0.245     \\\\\n",
       "\\textbf{x57}   &      -0.3631  &        0.003     &  -105.698  &         0.000        &       -0.370    &       -0.356     \\\\\n",
       "\\textbf{x58}   &      -0.0055  &        0.003     &    -1.685  &         0.092        &       -0.012    &        0.001     \\\\\n",
       "\\textbf{x59}   &       0.0035  &        0.003     &     1.054  &         0.292        &       -0.003    &        0.010     \\\\\n",
       "\\textbf{x60}   &       0.0179  &        0.004     &     4.590  &         0.000        &        0.010    &        0.026     \\\\\n",
       "\\textbf{x61}   &       0.0026  &        0.003     &     0.791  &         0.429        &       -0.004    &        0.009     \\\\\n",
       "\\textbf{x62}   &       0.0042  &        0.003     &     1.289  &         0.197        &       -0.002    &        0.011     \\\\\n",
       "\\textbf{x63}   &       0.0074  &        0.003     &     2.243  &         0.025        &        0.001    &        0.014     \\\\\n",
       "\\textbf{x64}   &      -0.0039  &        0.003     &    -1.189  &         0.234        &       -0.010    &        0.003     \\\\\n",
       "\\textbf{x65}   &       0.0026  &        0.003     &     0.780  &         0.435        &       -0.004    &        0.009     \\\\\n",
       "\\textbf{x66}   &      -0.0035  &        0.003     &    -1.043  &         0.297        &       -0.010    &        0.003     \\\\\n",
       "\\textbf{x67}   &      -0.0077  &        0.004     &    -1.816  &         0.069        &       -0.016    &        0.001     \\\\\n",
       "\\textbf{x68}   &      -0.0111  &        0.005     &    -2.172  &         0.030        &       -0.021    &       -0.001     \\\\\n",
       "\\textbf{x69}   &      -0.0122  &        0.004     &    -3.214  &         0.001        &       -0.020    &       -0.005     \\\\\n",
       "\\textbf{x70}   &      -0.0227  &        0.005     &    -4.689  &         0.000        &       -0.032    &       -0.013     \\\\\n",
       "\\textbf{x71}   &      -0.0181  &        0.005     &    -3.578  &         0.000        &       -0.028    &       -0.008     \\\\\n",
       "\\textbf{x72}   &      -0.0197  &        0.004     &    -4.509  &         0.000        &       -0.028    &       -0.011     \\\\\n",
       "\\textbf{x73}   &      -0.0304  &        0.005     &    -5.595  &         0.000        &       -0.041    &       -0.020     \\\\\n",
       "\\textbf{x74}   &      -0.0177  &        0.004     &    -4.420  &         0.000        &       -0.025    &       -0.010     \\\\\n",
       "\\textbf{x75}   &      -0.0085  &        0.004     &    -2.420  &         0.016        &       -0.015    &       -0.002     \\\\\n",
       "\\textbf{x76}   &      -0.0034  &        0.005     &    -0.755  &         0.450        &       -0.012    &        0.005     \\\\\n",
       "\\textbf{x77}   &      -0.0185  &        0.005     &    -3.659  &         0.000        &       -0.028    &       -0.009     \\\\\n",
       "\\textbf{x78}   &       0.0002  &        0.003     &     0.055  &         0.956        &       -0.007    &        0.007     \\\\\n",
       "\\textbf{x79}   &      -0.0031  &        0.003     &    -0.900  &         0.368        &       -0.010    &        0.004     \\\\\n",
       "\\textbf{x80}   &      -0.0095  &        0.004     &    -2.515  &         0.012        &       -0.017    &       -0.002     \\\\\n",
       "\\textbf{x81}   &      -0.0142  &        0.004     &    -3.241  &         0.001        &       -0.023    &       -0.006     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 12172.876 & \\textbf{  Durbin-Watson:     } &     2.003  \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 69343.759  \\\\\n",
       "\\textbf{Skew:}          &    0.528  & \\textbf{  Prob(JB):          } &      0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &    7.179  & \\textbf{  Cond. No.          } &      51.9  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:           DurationSecs   R-squared:                       0.187\n",
       "Model:                            OLS   Adj. R-squared:                  0.186\n",
       "Method:                 Least Squares   F-statistic:                     254.5\n",
       "Date:                Tue, 05 Nov 2024   Prob (F-statistic):               0.00\n",
       "Time:                        20:18:24   Log-Likelihood:            -1.2498e+05\n",
       "No. Observations:               89592   AIC:                         2.501e+05\n",
       "Df Residuals:                   89510   BIC:                         2.509e+05\n",
       "Df Model:                          81                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.5155      0.003   1077.245      0.000       3.509       3.522\n",
       "x1             0.0171      0.033      0.515      0.606      -0.048       0.082\n",
       "x2             0.2253      0.033      6.837      0.000       0.161       0.290\n",
       "x3            -0.0502      0.004    -11.285      0.000      -0.059      -0.041\n",
       "x4             0.2829      0.033      8.566      0.000       0.218       0.348\n",
       "x5             0.0880      0.006     15.198      0.000       0.077       0.099\n",
       "x6             0.0346      0.003     10.131      0.000       0.028       0.041\n",
       "x7            -0.1370      0.005    -28.244      0.000      -0.147      -0.127\n",
       "x8            -0.0148      0.004     -4.149      0.000      -0.022      -0.008\n",
       "x9             0.0171      0.004      4.058      0.000       0.009       0.025\n",
       "x10            0.0351      0.010      3.369      0.001       0.015       0.056\n",
       "x11            0.0629      0.004     14.713      0.000       0.055       0.071\n",
       "x12           -0.4380      0.060     -7.350      0.000      -0.555      -0.321\n",
       "x13           -0.0239      0.030     -0.798      0.425      -0.083       0.035\n",
       "x14            0.0696      0.006     11.062      0.000       0.057       0.082\n",
       "x15            0.0158      0.032      0.500      0.617      -0.046       0.078\n",
       "x16            0.0258      0.027      0.970      0.332      -0.026       0.078\n",
       "x17           -0.0191      0.003     -5.674      0.000      -0.026      -0.013\n",
       "x18            0.0211      0.008      2.612      0.009       0.005       0.037\n",
       "x19            0.0098      0.003      2.840      0.005       0.003       0.017\n",
       "x20           -0.0037      0.003     -1.083      0.279      -0.010       0.003\n",
       "x21           -0.0019      0.003     -0.555      0.579      -0.009       0.005\n",
       "x22            0.0139      0.004      3.682      0.000       0.006       0.021\n",
       "x23            0.0044      0.005      0.907      0.365      -0.005       0.014\n",
       "x24           -0.0092      0.004     -2.433      0.015      -0.017      -0.002\n",
       "x25           -0.0309      0.005     -6.074      0.000      -0.041      -0.021\n",
       "x26            0.0212      0.005      4.064      0.000       0.011       0.031\n",
       "x27            0.0157      0.005      3.305      0.001       0.006       0.025\n",
       "x28           -0.0015      0.005     -0.307      0.759      -0.011       0.008\n",
       "x29           -0.0067      0.005     -1.369      0.171      -0.016       0.003\n",
       "x30           -0.0086      0.005     -1.748      0.080      -0.018       0.001\n",
       "x31            0.0308      0.005      5.927      0.000       0.021       0.041\n",
       "x32            0.0230      0.005      4.658      0.000       0.013       0.033\n",
       "x33           -0.0214      0.005     -4.361      0.000      -0.031      -0.012\n",
       "x34            0.0215      0.005      4.378      0.000       0.012       0.031\n",
       "x35           -0.0062      0.005     -1.221      0.222      -0.016       0.004\n",
       "x36           -0.0161      0.005     -3.276      0.001      -0.026      -0.006\n",
       "x37           -0.0307      0.005     -6.114      0.000      -0.041      -0.021\n",
       "x38           -0.0090      0.005     -1.893      0.058      -0.018       0.000\n",
       "x39            0.0010      0.005      0.207      0.836      -0.008       0.010\n",
       "x40           -0.0427      0.004    -11.195      0.000      -0.050      -0.035\n",
       "x41           -0.0061      0.005     -1.189      0.235      -0.016       0.004\n",
       "x42           -0.0268      0.005     -5.175      0.000      -0.037      -0.017\n",
       "x43           -0.0030      0.005     -0.627      0.531      -0.012       0.006\n",
       "x44           -0.0166      0.005     -3.349      0.001      -0.026      -0.007\n",
       "x45           -0.0279      0.005     -5.681      0.000      -0.038      -0.018\n",
       "x46           -0.0215      0.005     -4.341      0.000      -0.031      -0.012\n",
       "x47           -0.0269      0.005     -5.175      0.000      -0.037      -0.017\n",
       "x48           -0.0037      0.005     -0.741      0.459      -0.014       0.006\n",
       "x49           -0.0310      0.005     -6.344      0.000      -0.041      -0.021\n",
       "x50            0.0106      0.005      2.185      0.029       0.001       0.020\n",
       "x51           -0.0128      0.005     -2.550      0.011      -0.023      -0.003\n",
       "x52            0.0018      0.005      0.363      0.716      -0.008       0.012\n",
       "x53            0.0007      0.005      0.146      0.884      -0.009       0.011\n",
       "x54           -0.0246      0.005     -5.113      0.000      -0.034      -0.015\n",
       "x55           -0.0038      0.003     -1.176      0.240      -0.010       0.003\n",
       "x56           -0.2512      0.003    -73.767      0.000      -0.258      -0.245\n",
       "x57           -0.3631      0.003   -105.698      0.000      -0.370      -0.356\n",
       "x58           -0.0055      0.003     -1.685      0.092      -0.012       0.001\n",
       "x59            0.0035      0.003      1.054      0.292      -0.003       0.010\n",
       "x60            0.0179      0.004      4.590      0.000       0.010       0.026\n",
       "x61            0.0026      0.003      0.791      0.429      -0.004       0.009\n",
       "x62            0.0042      0.003      1.289      0.197      -0.002       0.011\n",
       "x63            0.0074      0.003      2.243      0.025       0.001       0.014\n",
       "x64           -0.0039      0.003     -1.189      0.234      -0.010       0.003\n",
       "x65            0.0026      0.003      0.780      0.435      -0.004       0.009\n",
       "x66           -0.0035      0.003     -1.043      0.297      -0.010       0.003\n",
       "x67           -0.0077      0.004     -1.816      0.069      -0.016       0.001\n",
       "x68           -0.0111      0.005     -2.172      0.030      -0.021      -0.001\n",
       "x69           -0.0122      0.004     -3.214      0.001      -0.020      -0.005\n",
       "x70           -0.0227      0.005     -4.689      0.000      -0.032      -0.013\n",
       "x71           -0.0181      0.005     -3.578      0.000      -0.028      -0.008\n",
       "x72           -0.0197      0.004     -4.509      0.000      -0.028      -0.011\n",
       "x73           -0.0304      0.005     -5.595      0.000      -0.041      -0.020\n",
       "x74           -0.0177      0.004     -4.420      0.000      -0.025      -0.010\n",
       "x75           -0.0085      0.004     -2.420      0.016      -0.015      -0.002\n",
       "x76           -0.0034      0.005     -0.755      0.450      -0.012       0.005\n",
       "x77           -0.0185      0.005     -3.659      0.000      -0.028      -0.009\n",
       "x78            0.0002      0.003      0.055      0.956      -0.007       0.007\n",
       "x79           -0.0031      0.003     -0.900      0.368      -0.010       0.004\n",
       "x80           -0.0095      0.004     -2.515      0.012      -0.017      -0.002\n",
       "x81           -0.0142      0.004     -3.241      0.001      -0.023      -0.006\n",
       "==============================================================================\n",
       "Omnibus:                    12172.876   Durbin-Watson:                   2.003\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            69343.759\n",
       "Skew:                           0.528   Prob(JB):                         0.00\n",
       "Kurtosis:                       7.179   Cond. No.                         51.9\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_scaled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0ad0b301-73ef-4d5f-a33f-c08f76ed508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e02f7769-2014-49b2-bdcb-97804ad4199f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "55075098-c641-4343-bc98-44e36712962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "num_features=X_train.shape[1]\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "fad99ef9-faa1-4640-be72-04711be83e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Input(shape=(num_features,))) # the input layer where the shape of inputs is specified, for now it is just the number of features (more advanced inputs like image data would have more dimension)\n",
    "model.add(Dense(20, activation='relu')) \n",
    "model.add(Dense(20, kernel_regularizer=regularizers.l2(0.001), activation='relu')) \n",
    "model.add(Dense(1, activation='linear')) # the output layer has 1 unit, the linear activation is used as this is for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "db67c7e2-809e-4651-94ae-b4c20ec0ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='MSE', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "005a3624-52c5-45a1-a384-5fea39c8d663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m1,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,081</span> (8.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,081\u001b[0m (8.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,081</span> (8.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,081\u001b[0m (8.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "35c61be9-2ba2-4e42-b8f3-f42f837203ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 1.5550 - val_loss: 0.9723\n",
      "Epoch 2/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 984us/step - loss: 0.9536 - val_loss: 0.9537\n",
      "Epoch 3/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.9362 - val_loss: 0.9495\n",
      "Epoch 4/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 986us/step - loss: 0.9268 - val_loss: 0.9448\n",
      "Epoch 5/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.9210 - val_loss: 0.9426\n",
      "Epoch 6/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 982us/step - loss: 0.9169 - val_loss: 0.9403\n",
      "Epoch 7/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 965us/step - loss: 0.9137 - val_loss: 0.9379\n",
      "Epoch 8/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 996us/step - loss: 0.9110 - val_loss: 0.9368\n",
      "Epoch 9/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 927us/step - loss: 0.9086 - val_loss: 0.9362\n",
      "Epoch 10/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.9064 - val_loss: 0.9356\n",
      "Epoch 11/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.9045 - val_loss: 0.9346\n",
      "Epoch 12/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.9027 - val_loss: 0.9335\n",
      "Epoch 13/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.9007 - val_loss: 0.9328\n",
      "Epoch 14/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 975us/step - loss: 0.8990 - val_loss: 0.9324\n",
      "Epoch 15/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 993us/step - loss: 0.8973 - val_loss: 0.9325\n",
      "Epoch 16/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 940us/step - loss: 0.8954 - val_loss: 0.9328\n",
      "Epoch 17/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 996us/step - loss: 0.8939 - val_loss: 0.9322\n",
      "Epoch 18/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 950us/step - loss: 0.8928 - val_loss: 0.9326\n",
      "Epoch 19/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8922 - val_loss: 0.9326\n",
      "Epoch 20/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 950us/step - loss: 0.8919 - val_loss: 0.9324\n",
      "Epoch 21/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 997us/step - loss: 0.8912 - val_loss: 0.9322\n",
      "Epoch 22/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 970us/step - loss: 0.8906 - val_loss: 0.9324\n",
      "Epoch 23/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 962us/step - loss: 0.8903 - val_loss: 0.9322\n",
      "Epoch 24/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 990us/step - loss: 0.8899 - val_loss: 0.9321\n",
      "Epoch 25/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 953us/step - loss: 0.8895 - val_loss: 0.9321\n",
      "Epoch 26/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 998us/step - loss: 0.8891 - val_loss: 0.9319\n",
      "Epoch 27/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 969us/step - loss: 0.8887 - val_loss: 0.9319\n",
      "Epoch 28/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8885 - val_loss: 0.9318\n",
      "Epoch 29/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 984us/step - loss: 0.8882 - val_loss: 0.9319\n",
      "Epoch 30/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 999us/step - loss: 0.8881 - val_loss: 0.9318\n",
      "Epoch 31/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 969us/step - loss: 0.8879 - val_loss: 0.9312\n",
      "Epoch 32/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 991us/step - loss: 0.8874 - val_loss: 0.9310\n",
      "Epoch 33/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 998us/step - loss: 0.8871 - val_loss: 0.9316\n",
      "Epoch 34/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 979us/step - loss: 0.8868 - val_loss: 0.9314\n",
      "Epoch 35/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8867 - val_loss: 0.9309\n",
      "Epoch 36/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 960us/step - loss: 0.8862 - val_loss: 0.9312\n",
      "Epoch 37/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8859 - val_loss: 0.9309\n",
      "Epoch 38/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 956us/step - loss: 0.8858 - val_loss: 0.9312\n",
      "Epoch 39/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8854 - val_loss: 0.9321\n",
      "Epoch 40/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 974us/step - loss: 0.8853 - val_loss: 0.9316\n",
      "Epoch 41/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8853 - val_loss: 0.9329\n",
      "Epoch 42/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8851 - val_loss: 0.9328\n",
      "Epoch 43/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 977us/step - loss: 0.8848 - val_loss: 0.9327\n",
      "Epoch 44/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8845 - val_loss: 0.9332\n",
      "Epoch 45/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 967us/step - loss: 0.8842 - val_loss: 0.9334\n",
      "Epoch 46/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8842 - val_loss: 0.9332\n",
      "Epoch 47/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 985us/step - loss: 0.8840 - val_loss: 0.9331\n",
      "Epoch 48/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8837 - val_loss: 0.9331\n",
      "Epoch 49/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 981us/step - loss: 0.8835 - val_loss: 0.9327\n",
      "Epoch 50/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 970us/step - loss: 0.8832 - val_loss: 0.9328\n",
      "Epoch 51/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8830 - val_loss: 0.9326\n",
      "Epoch 52/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 978us/step - loss: 0.8831 - val_loss: 0.9323\n",
      "Epoch 53/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8826 - val_loss: 0.9319\n",
      "Epoch 54/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 978us/step - loss: 0.8826 - val_loss: 0.9321\n",
      "Epoch 55/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8825 - val_loss: 0.9331\n",
      "Epoch 56/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8821 - val_loss: 0.9326\n",
      "Epoch 57/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 981us/step - loss: 0.8819 - val_loss: 0.9322\n",
      "Epoch 58/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 993us/step - loss: 0.8818 - val_loss: 0.9323\n",
      "Epoch 59/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 956us/step - loss: 0.8815 - val_loss: 0.9322\n",
      "Epoch 60/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8814 - val_loss: 0.9327\n",
      "Epoch 61/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 948us/step - loss: 0.8811 - val_loss: 0.9327\n",
      "Epoch 62/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 997us/step - loss: 0.8810 - val_loss: 0.9329\n",
      "Epoch 63/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 958us/step - loss: 0.8809 - val_loss: 0.9333\n",
      "Epoch 64/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 999us/step - loss: 0.8807 - val_loss: 0.9320\n",
      "Epoch 65/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 954us/step - loss: 0.8805 - val_loss: 0.9320\n",
      "Epoch 66/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8804 - val_loss: 0.9328\n",
      "Epoch 67/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 975us/step - loss: 0.8803 - val_loss: 0.9324\n",
      "Epoch 68/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 972us/step - loss: 0.8801 - val_loss: 0.9323\n",
      "Epoch 69/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 988us/step - loss: 0.8802 - val_loss: 0.9329\n",
      "Epoch 70/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 973us/step - loss: 0.8800 - val_loss: 0.9322\n",
      "Epoch 71/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8800 - val_loss: 0.9322\n",
      "Epoch 72/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 969us/step - loss: 0.8800 - val_loss: 0.9323\n",
      "Epoch 73/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 992us/step - loss: 0.8799 - val_loss: 0.9322\n",
      "Epoch 74/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 947us/step - loss: 0.8797 - val_loss: 0.9321\n",
      "Epoch 75/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 989us/step - loss: 0.8794 - val_loss: 0.9327\n",
      "Epoch 76/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 955us/step - loss: 0.8794 - val_loss: 0.9328\n",
      "Epoch 77/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 979us/step - loss: 0.8792 - val_loss: 0.9324\n",
      "Epoch 78/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 975us/step - loss: 0.8793 - val_loss: 0.9325\n",
      "Epoch 79/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 966us/step - loss: 0.8792 - val_loss: 0.9325\n",
      "Epoch 80/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 984us/step - loss: 0.8789 - val_loss: 0.9325\n",
      "Epoch 81/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 977us/step - loss: 0.8790 - val_loss: 0.9327\n",
      "Epoch 82/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 989us/step - loss: 0.8790 - val_loss: 0.9326\n",
      "Epoch 83/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 948us/step - loss: 0.8788 - val_loss: 0.9327\n",
      "Epoch 84/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8788 - val_loss: 0.9328\n",
      "Epoch 85/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 973us/step - loss: 0.8787 - val_loss: 0.9331\n",
      "Epoch 86/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 999us/step - loss: 0.8788 - val_loss: 0.9334\n",
      "Epoch 87/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 962us/step - loss: 0.8787 - val_loss: 0.9337\n",
      "Epoch 88/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 967us/step - loss: 0.8785 - val_loss: 0.9333\n",
      "Epoch 89/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8784 - val_loss: 0.9338\n",
      "Epoch 90/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 968us/step - loss: 0.8782 - val_loss: 0.9339\n",
      "Epoch 91/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 990us/step - loss: 0.8781 - val_loss: 0.9337\n",
      "Epoch 92/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 959us/step - loss: 0.8779 - val_loss: 0.9340\n",
      "Epoch 93/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8781 - val_loss: 0.9340\n",
      "Epoch 94/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 942us/step - loss: 0.8780 - val_loss: 0.9339\n",
      "Epoch 95/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 984us/step - loss: 0.8780 - val_loss: 0.9338\n",
      "Epoch 96/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 951us/step - loss: 0.8779 - val_loss: 0.9339\n",
      "Epoch 97/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 982us/step - loss: 0.8779 - val_loss: 0.9343\n",
      "Epoch 98/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 981us/step - loss: 0.8776 - val_loss: 0.9341\n",
      "Epoch 99/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 962us/step - loss: 0.8777 - val_loss: 0.9342\n",
      "Epoch 100/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 999us/step - loss: 0.8774 - val_loss: 0.9341\n"
     ]
    }
   ],
   "source": [
    "modelout = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6d77e3b4-6ca0-4d2a-b0af-0b7776b2c974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(modelout.history['loss'], label='Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "97c39c9c-e665-4a61-85c1-77c6418ac0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE on the test data for neural net:  0.9341298341751099\n",
      "MSE on the test data for linear reg:  0.9547626863323189\n"
     ]
    }
   ],
   "source": [
    "MSE_nn = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('\\nMSE on the test data for neural net: ', MSE_nn)\n",
    "print('MSE on the test data for linear reg: ', MSE_lm_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
