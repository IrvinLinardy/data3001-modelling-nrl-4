{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c7174cd-9e93-4fc8-87f8-1766f763849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jeffr\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ccbd0f86-4734-4402-85f3-f841f2e72f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "801f8d02-2d15-4eed-9381-610fde107a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_data = pd.read_csv(r'C:\\Users\\jeffr\\Downloads\\ptb_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ee14e7e8-4e68-4274-96e7-57e3eb556331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 111990 entries, 0 to 112026\n",
      "Data columns (total 82 columns):\n",
      " #   Column                                             Non-Null Count   Dtype  \n",
      "---  ------                                             --------------   -----  \n",
      " 0   DurationSecs                                       111990 non-null  float64\n",
      " 1   Half                                               111990 non-null  int64  \n",
      " 2   OppPossessionSecs                                  111990 non-null  float64\n",
      " 3   PositionId                                         111990 non-null  float64\n",
      " 4   PossessionSecs                                     111990 non-null  float64\n",
      " 5   Set Type                                           111990 non-null  int64  \n",
      " 6   Total Involved Tacklers                            111990 non-null  float64\n",
      " 7   Raw Tackle Number                                  111990 non-null  float64\n",
      " 8   RoundId                                            111990 non-null  int64  \n",
      " 9   RunOn                                              111990 non-null  int64  \n",
      " 10  Score                                              111990 non-null  float64\n",
      " 11  SeasonId                                           111990 non-null  int64  \n",
      " 12  SeqNumber                                          111990 non-null  int64  \n",
      " 13  Set                                                111990 non-null  int64  \n",
      " 14  ZonePossession                                     111990 non-null  int64  \n",
      " 15  GameTime                                           111990 non-null  float64\n",
      " 16  ElapsedTime                                        111990 non-null  float64\n",
      " 17  IsHome                                             111990 non-null  int64  \n",
      " 18  CurrentMargin                                      111990 non-null  float64\n",
      " 19  WeatherConditionName_Rain                          111990 non-null  int32  \n",
      " 20  WeatherConditionName_Showers                       111990 non-null  int32  \n",
      " 21  WeatherConditionName_Snow                          111990 non-null  int32  \n",
      " 22  WeatherConditionName_Unknown                       111990 non-null  int32  \n",
      " 23  Club Id_1d6cd83892ee4afdcd8ccd94f817b4a6           111990 non-null  int32  \n",
      " 24  Club Id_1d6cd83892ee4afdcd8ccd94f81ftnjhl3s        111990 non-null  int32  \n",
      " 25  Club Id_367ef61d2bc259e608027a8d349c933e           111990 non-null  int32  \n",
      " 26  Club Id_3b26834df063f9d51de216a07ec36929           111990 non-null  int32  \n",
      " 27  Club Id_58485e3acf60682c8fc37d9d521b3019           111990 non-null  int32  \n",
      " 28  Club Id_5e03a19f4d014a2220665cfd56522d35           111990 non-null  int32  \n",
      " 29  Club Id_837e03d56b4dba3b8a4a5425c0420abd           111990 non-null  int32  \n",
      " 30  Club Id_980c9c368ae4f1129ea0a6fdd711fa8f           111990 non-null  int32  \n",
      " 31  Club Id_a73752d38e4a78e3e14917f5435ffb6d           111990 non-null  int32  \n",
      " 32  Club Id_b53920c88e4eebf2faa9f4fb43b8944a           111990 non-null  int32  \n",
      " 33  Club Id_c03196722c1a837b39f79f1714db475d           111990 non-null  int32  \n",
      " 34  Club Id_c14e0139ad91a9741a5731a596aa6549           111990 non-null  int32  \n",
      " 35  Club Id_d3ac47d424b41fd738ec9500dbda2d59           111990 non-null  int32  \n",
      " 36  Club Id_dc3c7bd8148814b7c4105841baa68e23           111990 non-null  int32  \n",
      " 37  Club Id_f38f7f087f646c38c0207f1b2af32f12           111990 non-null  int32  \n",
      " 38  Club Id_fdfcde48e2cbf12cc4710a2644b86d85           111990 non-null  int32  \n",
      " 39  Opposition Id_1d6cd83892ee4afdcd8ccd94f817b4a6     111990 non-null  int32  \n",
      " 40  Opposition Id_1d6cd83892ee4afdcd8ccd94f81ftnjhl3s  111990 non-null  int32  \n",
      " 41  Opposition Id_367ef61d2bc259e608027a8d349c933e     111990 non-null  int32  \n",
      " 42  Opposition Id_3b26834df063f9d51de216a07ec36929     111990 non-null  int32  \n",
      " 43  Opposition Id_58485e3acf60682c8fc37d9d521b3019     111990 non-null  int32  \n",
      " 44  Opposition Id_5e03a19f4d014a2220665cfd56522d35     111990 non-null  int32  \n",
      " 45  Opposition Id_837e03d56b4dba3b8a4a5425c0420abd     111990 non-null  int32  \n",
      " 46  Opposition Id_980c9c368ae4f1129ea0a6fdd711fa8f     111990 non-null  int32  \n",
      " 47  Opposition Id_a73752d38e4a78e3e14917f5435ffb6d     111990 non-null  int32  \n",
      " 48  Opposition Id_b53920c88e4eebf2faa9f4fb43b8944a     111990 non-null  int32  \n",
      " 49  Opposition Id_c03196722c1a837b39f79f1714db475d     111990 non-null  int32  \n",
      " 50  Opposition Id_c14e0139ad91a9741a5731a596aa6549     111990 non-null  int32  \n",
      " 51  Opposition Id_d3ac47d424b41fd738ec9500dbda2d59     111990 non-null  int32  \n",
      " 52  Opposition Id_dc3c7bd8148814b7c4105841baa68e23     111990 non-null  int32  \n",
      " 53  Opposition Id_f38f7f087f646c38c0207f1b2af32f12     111990 non-null  int32  \n",
      " 54  Opposition Id_fdfcde48e2cbf12cc4710a2644b86d85     111990 non-null  int32  \n",
      " 55  PTB Contest_Other                                  111990 non-null  int32  \n",
      " 56  PTB Contest_Stays on feet                          111990 non-null  int32  \n",
      " 57  PTB Contest_Tackled to ground                      111990 non-null  int32  \n",
      " 58  PTB Ultimate Outcome_Field goal attempt            111990 non-null  int32  \n",
      " 59  PTB Ultimate Outcome_Handover                      111990 non-null  int32  \n",
      " 60  PTB Ultimate Outcome_Kick                          111990 non-null  int32  \n",
      " 61  PTB Ultimate Outcome_Opp ruck infringement         111990 non-null  int32  \n",
      " 62  PTB Ultimate Outcome_Other                         111990 non-null  int32  \n",
      " 63  PTB Ultimate Outcome_Penalty attack                111990 non-null  int32  \n",
      " 64  PTB Ultimate Outcome_Penalty defence               111990 non-null  int32  \n",
      " 65  PTB Ultimate Outcome_Try                           111990 non-null  int32  \n",
      " 66  PTB Ultimate Outcome_Turnover                      111990 non-null  int32  \n",
      " 67  OfficialId_500002                                  111990 non-null  int32  \n",
      " 68  OfficialId_500004                                  111990 non-null  int32  \n",
      " 69  OfficialId_500006                                  111990 non-null  int32  \n",
      " 70  OfficialId_500016                                  111990 non-null  int32  \n",
      " 71  OfficialId_500019                                  111990 non-null  int32  \n",
      " 72  OfficialId_500029                                  111990 non-null  int32  \n",
      " 73  OfficialId_500039                                  111990 non-null  int32  \n",
      " 74  OfficialId_500042                                  111990 non-null  int32  \n",
      " 75  OfficialId_500045                                  111990 non-null  int32  \n",
      " 76  OfficialId_500050                                  111990 non-null  int32  \n",
      " 77  OfficialId_500067                                  111990 non-null  int32  \n",
      " 78  OfficialId_500070                                  111990 non-null  int32  \n",
      " 79  OfficialId_500072                                  111990 non-null  int32  \n",
      " 80  OfficialId_500075                                  111990 non-null  int32  \n",
      " 81  OfficialId_500117                                  111990 non-null  int32  \n",
      "dtypes: float64(10), int32(63), int64(9)\n",
      "memory usage: 44.0 MB\n"
     ]
    }
   ],
   "source": [
    "# df_encoded = ptb_data.drop(columns = ['PTB Defence','Anonymize 1PlayerId', 'EventName', 'Club Id', 'Opposition Id', 'PTB Contest', \n",
    "#                                         'PTB Ultimate Outcome', 'WeatherConditionName'])\n",
    "\n",
    "\n",
    "df_encoded = ptb_data.drop(columns = ['PTB Defence', 'Anonymize 1PlayerId', 'Player Id', 'ZonePhysical', 'MatchId', 'Tackle', 'OppScore', 'Home Score', 'Away Score', 'PTB Tackle Result', 'EventName', 'TotalPossessionSecs'])\n",
    "\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['WeatherConditionName', 'Club Id', 'Opposition Id', 'PTB Contest', 'PTB Ultimate Outcome', 'OfficialId'], drop_first=True)\n",
    "\n",
    "\n",
    "df_encoded = df_encoded.dropna()\n",
    "\n",
    "df_encoded[df_encoded.select_dtypes(include=['bool']).columns] = df_encoded.select_dtypes(include=['bool']).astype(int)\n",
    "\n",
    "df_encoded.info()\n",
    "\n",
    "df_encoded.to_csv('encoded_ptb_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ef807a8c-bc4a-4d8e-97f3-b30c7a12257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50654410-0e0d-4bf4-944c-c737003cd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6fcc34fd-4f44-4edc-a4ef-e09f0f107479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>DurationSecs</td>   <th>  R-squared:         </th>  <td>   0.005</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.005</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   288.2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 05 Nov 2024</td> <th>  Prob (F-statistic):</th>  <td>1.46e-125</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:05:43</td>     <th>  Log-Likelihood:    </th> <td>-1.6760e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>111990</td>      <th>  AIC:               </th>  <td>3.352e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>111987</td>      <th>  BIC:               </th>  <td>3.352e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>         <td>    3.4071</td> <td>    0.011</td> <td>  303.729</td> <td> 0.000</td> <td>    3.385</td> <td>    3.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Half</th>              <td>   -0.0240</td> <td>    0.012</td> <td>   -2.038</td> <td> 0.042</td> <td>   -0.047</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OppPossessionSecs</th> <td>    0.0002</td> <td> 1.23e-05</td> <td>   14.573</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13105.824</td> <th>  Durbin-Watson:     </th> <td>   1.826</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>47357.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.571</td>   <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.974</td>   <th>  Cond. No.          </th> <td>4.57e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.57e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &   DurationSecs   & \\textbf{  R-squared:         } &      0.005   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.005   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      288.2   \\\\\n",
       "\\textbf{Date:}             & Tue, 05 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.46e-125   \\\\\n",
       "\\textbf{Time:}             &     20:05:43     & \\textbf{  Log-Likelihood:    } & -1.6760e+05  \\\\\n",
       "\\textbf{No. Observations:} &      111990      & \\textbf{  AIC:               } &  3.352e+05   \\\\\n",
       "\\textbf{Df Residuals:}     &      111987      & \\textbf{  BIC:               } &  3.352e+05   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &              \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &              \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}         &       3.4071  &        0.011     &   303.729  &         0.000        &        3.385    &        3.429     \\\\\n",
       "\\textbf{Half}              &      -0.0240  &        0.012     &    -2.038  &         0.042        &       -0.047    &       -0.001     \\\\\n",
       "\\textbf{OppPossessionSecs} &       0.0002  &     1.23e-05     &    14.573  &         0.000        &        0.000    &        0.000     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 13105.824 & \\textbf{  Durbin-Watson:     } &     1.826  \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 47357.703  \\\\\n",
       "\\textbf{Skew:}          &    0.571  & \\textbf{  Prob(JB):          } &      0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &    5.974  & \\textbf{  Cond. No.          } &  4.57e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 4.57e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:           DurationSecs   R-squared:                       0.005\n",
       "Model:                            OLS   Adj. R-squared:                  0.005\n",
       "Method:                 Least Squares   F-statistic:                     288.2\n",
       "Date:                Tue, 05 Nov 2024   Prob (F-statistic):          1.46e-125\n",
       "Time:                        20:05:43   Log-Likelihood:            -1.6760e+05\n",
       "No. Observations:              111990   AIC:                         3.352e+05\n",
       "Df Residuals:                  111987   BIC:                         3.352e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================\n",
       "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------\n",
       "Intercept             3.4071      0.011    303.729      0.000       3.385       3.429\n",
       "Half                 -0.0240      0.012     -2.038      0.042      -0.047      -0.001\n",
       "OppPossessionSecs     0.0002   1.23e-05     14.573      0.000       0.000       0.000\n",
       "==============================================================================\n",
       "Omnibus:                    13105.824   Durbin-Watson:                   1.826\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            47357.703\n",
       "Skew:                           0.571   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.974   Cond. No.                     4.57e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.57e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrl_train, nrl_test = train_test_split(df_encoded, test_size = 0.2,random_state=1)\n",
    "df_encoded.columns = df_encoded.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "\n",
    "formula = 'DurationSecs ~ ' + ' + '.join(df_encoded.columns.difference(['DurationSecs']))\n",
    "model = smf.ols(formula=formula, data=df_encoded).fit()\n",
    "predictions = lm.predict(nrl_test)\n",
    "y_test = nrl_test.DurationSecs\n",
    "y_train = nrl_train.DurationSecs\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "51c05bd2-adab-4077-8640-143f4d9bd27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the test data for linear regression:  1.1740740381255725\n"
     ]
    }
   ],
   "source": [
    "MSE_lm = np.mean((predictions-y_test)**2)\n",
    "print('MSE on the test data for linear regression: ',MSE_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5db66d73-4213-4a86-b260-94d6fe9f16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = nrl_train.drop('DurationSecs',axis='columns')\n",
    "scaler.fit(X_train) #fit scaler (estimate mean and std) on training data only!\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# apply the same transformation to test data\n",
    "X_test = nrl_test.drop('DurationSecs',axis='columns')\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "29995c16-c2fd-4f5f-b30c-e2f469b7d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.986388683257667e-17 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:,1].mean(),X_train[:,1].std()) #check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4d1a2c58-8308-4654-9b53-70606423d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the test data for linear regression:  0.954762686332319\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_1 = sm.add_constant(X_train)\n",
    "X_test_1 = sm.add_constant(X_test)\n",
    "model_sc = sm.OLS(y_train, X_train_1)\n",
    "lm_scaled = model_sc.fit()\n",
    "\n",
    "lm_scaled.summary()\n",
    "\n",
    "predictions_scaled = lm_scaled.predict(X_test_1)\n",
    "\n",
    "MSE_lm_scaled = np.mean((predictions_scaled-y_test)**2)\n",
    "print('MSE on the test data for linear regression: ',MSE_lm_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6422b80e-d1d5-4b1b-a22d-fb2cfb2a48d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>DurationSecs</td>   <th>  R-squared:         </th>  <td>   0.187</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.186</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   254.5</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 05 Nov 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:19:04</td>     <th>  Log-Likelihood:    </th> <td>-1.2498e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 89592</td>      <th>  AIC:               </th>  <td>2.501e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 89510</td>      <th>  BIC:               </th>  <td>2.509e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    81</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    3.5155</td> <td>    0.003</td> <td> 1077.245</td> <td> 0.000</td> <td>    3.509</td> <td>    3.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.0171</td> <td>    0.033</td> <td>    0.515</td> <td> 0.606</td> <td>   -0.048</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.1386</td> <td>    0.024</td> <td>    5.888</td> <td> 0.000</td> <td>    0.092</td> <td>    0.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0502</td> <td>    0.004</td> <td>  -11.285</td> <td> 0.000</td> <td>   -0.059</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.1960</td> <td>    0.024</td> <td>    8.310</td> <td> 0.000</td> <td>    0.150</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0880</td> <td>    0.006</td> <td>   15.198</td> <td> 0.000</td> <td>    0.077</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0346</td> <td>    0.003</td> <td>   10.131</td> <td> 0.000</td> <td>    0.028</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.1370</td> <td>    0.005</td> <td>  -28.244</td> <td> 0.000</td> <td>   -0.147</td> <td>   -0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.0148</td> <td>    0.004</td> <td>   -4.149</td> <td> 0.000</td> <td>   -0.022</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0171</td> <td>    0.004</td> <td>    4.058</td> <td> 0.000</td> <td>    0.009</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.0351</td> <td>    0.010</td> <td>    3.369</td> <td> 0.001</td> <td>    0.015</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.0629</td> <td>    0.004</td> <td>   14.713</td> <td> 0.000</td> <td>    0.055</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   -0.4380</td> <td>    0.060</td> <td>   -7.350</td> <td> 0.000</td> <td>   -0.555</td> <td>   -0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0239</td> <td>    0.030</td> <td>   -0.798</td> <td> 0.425</td> <td>   -0.083</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.1704</td> <td>    0.021</td> <td>    8.285</td> <td> 0.000</td> <td>    0.130</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    0.0696</td> <td>    0.006</td> <td>   11.062</td> <td> 0.000</td> <td>    0.057</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.0158</td> <td>    0.032</td> <td>    0.500</td> <td> 0.617</td> <td>   -0.046</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    0.0258</td> <td>    0.027</td> <td>    0.970</td> <td> 0.332</td> <td>   -0.026</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.0191</td> <td>    0.003</td> <td>   -5.674</td> <td> 0.000</td> <td>   -0.026</td> <td>   -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0211</td> <td>    0.008</td> <td>    2.612</td> <td> 0.009</td> <td>    0.005</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    0.0098</td> <td>    0.003</td> <td>    2.840</td> <td> 0.005</td> <td>    0.003</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -0.0037</td> <td>    0.003</td> <td>   -1.083</td> <td> 0.279</td> <td>   -0.010</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>   -0.0019</td> <td>    0.003</td> <td>   -0.555</td> <td> 0.579</td> <td>   -0.009</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.0139</td> <td>    0.004</td> <td>    3.682</td> <td> 0.000</td> <td>    0.006</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0044</td> <td>    0.005</td> <td>    0.907</td> <td> 0.365</td> <td>   -0.005</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.0092</td> <td>    0.004</td> <td>   -2.433</td> <td> 0.015</td> <td>   -0.017</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0309</td> <td>    0.005</td> <td>   -6.074</td> <td> 0.000</td> <td>   -0.041</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    0.0212</td> <td>    0.005</td> <td>    4.064</td> <td> 0.000</td> <td>    0.011</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>    0.0157</td> <td>    0.005</td> <td>    3.305</td> <td> 0.001</td> <td>    0.006</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.0015</td> <td>    0.005</td> <td>   -0.307</td> <td> 0.759</td> <td>   -0.011</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -0.0067</td> <td>    0.005</td> <td>   -1.369</td> <td> 0.171</td> <td>   -0.016</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>   -0.0086</td> <td>    0.005</td> <td>   -1.748</td> <td> 0.080</td> <td>   -0.018</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>    0.0308</td> <td>    0.005</td> <td>    5.927</td> <td> 0.000</td> <td>    0.021</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.0230</td> <td>    0.005</td> <td>    4.658</td> <td> 0.000</td> <td>    0.013</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>   -0.0214</td> <td>    0.005</td> <td>   -4.361</td> <td> 0.000</td> <td>   -0.031</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.0215</td> <td>    0.005</td> <td>    4.378</td> <td> 0.000</td> <td>    0.012</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.0062</td> <td>    0.005</td> <td>   -1.221</td> <td> 0.222</td> <td>   -0.016</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>   -0.0161</td> <td>    0.005</td> <td>   -3.276</td> <td> 0.001</td> <td>   -0.026</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>   -0.0307</td> <td>    0.005</td> <td>   -6.114</td> <td> 0.000</td> <td>   -0.041</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   -0.0090</td> <td>    0.005</td> <td>   -1.893</td> <td> 0.058</td> <td>   -0.018</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    0.0010</td> <td>    0.005</td> <td>    0.207</td> <td> 0.836</td> <td>   -0.008</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>   -0.0427</td> <td>    0.004</td> <td>  -11.195</td> <td> 0.000</td> <td>   -0.050</td> <td>   -0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>   -0.0061</td> <td>    0.005</td> <td>   -1.189</td> <td> 0.235</td> <td>   -0.016</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>   -0.0268</td> <td>    0.005</td> <td>   -5.175</td> <td> 0.000</td> <td>   -0.037</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>   -0.0030</td> <td>    0.005</td> <td>   -0.627</td> <td> 0.531</td> <td>   -0.012</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -0.0166</td> <td>    0.005</td> <td>   -3.349</td> <td> 0.001</td> <td>   -0.026</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   -0.0279</td> <td>    0.005</td> <td>   -5.681</td> <td> 0.000</td> <td>   -0.038</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>   -0.0215</td> <td>    0.005</td> <td>   -4.341</td> <td> 0.000</td> <td>   -0.031</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>   -0.0269</td> <td>    0.005</td> <td>   -5.175</td> <td> 0.000</td> <td>   -0.037</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>   -0.0037</td> <td>    0.005</td> <td>   -0.741</td> <td> 0.459</td> <td>   -0.014</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>   -0.0310</td> <td>    0.005</td> <td>   -6.344</td> <td> 0.000</td> <td>   -0.041</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>    0.0106</td> <td>    0.005</td> <td>    2.185</td> <td> 0.029</td> <td>    0.001</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>   -0.0128</td> <td>    0.005</td> <td>   -2.550</td> <td> 0.011</td> <td>   -0.023</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    0.0018</td> <td>    0.005</td> <td>    0.363</td> <td> 0.716</td> <td>   -0.008</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>    0.0007</td> <td>    0.005</td> <td>    0.146</td> <td> 0.884</td> <td>   -0.009</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>   -0.0246</td> <td>    0.005</td> <td>   -5.113</td> <td> 0.000</td> <td>   -0.034</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>   -0.0038</td> <td>    0.003</td> <td>   -1.176</td> <td> 0.240</td> <td>   -0.010</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>   -0.2512</td> <td>    0.003</td> <td>  -73.767</td> <td> 0.000</td> <td>   -0.258</td> <td>   -0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>   -0.3631</td> <td>    0.003</td> <td> -105.698</td> <td> 0.000</td> <td>   -0.370</td> <td>   -0.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>   -0.0055</td> <td>    0.003</td> <td>   -1.685</td> <td> 0.092</td> <td>   -0.012</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>    0.0035</td> <td>    0.003</td> <td>    1.054</td> <td> 0.292</td> <td>   -0.003</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>    0.0179</td> <td>    0.004</td> <td>    4.590</td> <td> 0.000</td> <td>    0.010</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>    0.0026</td> <td>    0.003</td> <td>    0.791</td> <td> 0.429</td> <td>   -0.004</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>    0.0042</td> <td>    0.003</td> <td>    1.289</td> <td> 0.197</td> <td>   -0.002</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>    0.0074</td> <td>    0.003</td> <td>    2.243</td> <td> 0.025</td> <td>    0.001</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>   -0.0039</td> <td>    0.003</td> <td>   -1.189</td> <td> 0.234</td> <td>   -0.010</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>    0.0026</td> <td>    0.003</td> <td>    0.780</td> <td> 0.435</td> <td>   -0.004</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>   -0.0035</td> <td>    0.003</td> <td>   -1.043</td> <td> 0.297</td> <td>   -0.010</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>   -0.0077</td> <td>    0.004</td> <td>   -1.816</td> <td> 0.069</td> <td>   -0.016</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>   -0.0111</td> <td>    0.005</td> <td>   -2.172</td> <td> 0.030</td> <td>   -0.021</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>   -0.0122</td> <td>    0.004</td> <td>   -3.214</td> <td> 0.001</td> <td>   -0.020</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>   -0.0227</td> <td>    0.005</td> <td>   -4.689</td> <td> 0.000</td> <td>   -0.032</td> <td>   -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>   -0.0181</td> <td>    0.005</td> <td>   -3.578</td> <td> 0.000</td> <td>   -0.028</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>   -0.0197</td> <td>    0.004</td> <td>   -4.509</td> <td> 0.000</td> <td>   -0.028</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>   -0.0304</td> <td>    0.005</td> <td>   -5.595</td> <td> 0.000</td> <td>   -0.041</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>   -0.0177</td> <td>    0.004</td> <td>   -4.420</td> <td> 0.000</td> <td>   -0.025</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>   -0.0085</td> <td>    0.004</td> <td>   -2.420</td> <td> 0.016</td> <td>   -0.015</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>   -0.0034</td> <td>    0.005</td> <td>   -0.755</td> <td> 0.450</td> <td>   -0.012</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>   -0.0185</td> <td>    0.005</td> <td>   -3.659</td> <td> 0.000</td> <td>   -0.028</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>    0.0002</td> <td>    0.003</td> <td>    0.055</td> <td> 0.956</td> <td>   -0.007</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>   -0.0031</td> <td>    0.003</td> <td>   -0.900</td> <td> 0.368</td> <td>   -0.010</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>   -0.0095</td> <td>    0.004</td> <td>   -2.515</td> <td> 0.012</td> <td>   -0.017</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>   -0.0142</td> <td>    0.004</td> <td>   -3.241</td> <td> 0.001</td> <td>   -0.023</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>12172.876</td> <th>  Durbin-Watson:     </th> <td>   2.003</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>69343.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.528</td>   <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 7.179</td>   <th>  Cond. No.          </th> <td>4.57e+15</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.85e-26. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &   DurationSecs   & \\textbf{  R-squared:         } &      0.187   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.186   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      254.5   \\\\\n",
       "\\textbf{Date:}             & Tue, 05 Nov 2024 & \\textbf{  Prob (F-statistic):} &      0.00    \\\\\n",
       "\\textbf{Time:}             &     19:19:04     & \\textbf{  Log-Likelihood:    } & -1.2498e+05  \\\\\n",
       "\\textbf{No. Observations:} &       89592      & \\textbf{  AIC:               } &  2.501e+05   \\\\\n",
       "\\textbf{Df Residuals:}     &       89510      & \\textbf{  BIC:               } &  2.509e+05   \\\\\n",
       "\\textbf{Df Model:}         &          81      & \\textbf{                     } &              \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &              \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &       3.5155  &        0.003     &  1077.245  &         0.000        &        3.509    &        3.522     \\\\\n",
       "\\textbf{x1}    &       0.0171  &        0.033     &     0.515  &         0.606        &       -0.048    &        0.082     \\\\\n",
       "\\textbf{x2}    &       0.1386  &        0.024     &     5.888  &         0.000        &        0.092    &        0.185     \\\\\n",
       "\\textbf{x3}    &      -0.0502  &        0.004     &   -11.285  &         0.000        &       -0.059    &       -0.041     \\\\\n",
       "\\textbf{x4}    &       0.1960  &        0.024     &     8.310  &         0.000        &        0.150    &        0.242     \\\\\n",
       "\\textbf{x5}    &       0.0880  &        0.006     &    15.198  &         0.000        &        0.077    &        0.099     \\\\\n",
       "\\textbf{x6}    &       0.0346  &        0.003     &    10.131  &         0.000        &        0.028    &        0.041     \\\\\n",
       "\\textbf{x7}    &      -0.1370  &        0.005     &   -28.244  &         0.000        &       -0.147    &       -0.127     \\\\\n",
       "\\textbf{x8}    &      -0.0148  &        0.004     &    -4.149  &         0.000        &       -0.022    &       -0.008     \\\\\n",
       "\\textbf{x9}    &       0.0171  &        0.004     &     4.058  &         0.000        &        0.009    &        0.025     \\\\\n",
       "\\textbf{x10}   &       0.0351  &        0.010     &     3.369  &         0.001        &        0.015    &        0.056     \\\\\n",
       "\\textbf{x11}   &       0.0629  &        0.004     &    14.713  &         0.000        &        0.055    &        0.071     \\\\\n",
       "\\textbf{x12}   &      -0.4380  &        0.060     &    -7.350  &         0.000        &       -0.555    &       -0.321     \\\\\n",
       "\\textbf{x13}   &      -0.0239  &        0.030     &    -0.798  &         0.425        &       -0.083    &        0.035     \\\\\n",
       "\\textbf{x14}   &       0.1704  &        0.021     &     8.285  &         0.000        &        0.130    &        0.211     \\\\\n",
       "\\textbf{x15}   &       0.0696  &        0.006     &    11.062  &         0.000        &        0.057    &        0.082     \\\\\n",
       "\\textbf{x16}   &       0.0158  &        0.032     &     0.500  &         0.617        &       -0.046    &        0.078     \\\\\n",
       "\\textbf{x17}   &       0.0258  &        0.027     &     0.970  &         0.332        &       -0.026    &        0.078     \\\\\n",
       "\\textbf{x18}   &      -0.0191  &        0.003     &    -5.674  &         0.000        &       -0.026    &       -0.013     \\\\\n",
       "\\textbf{x19}   &       0.0211  &        0.008     &     2.612  &         0.009        &        0.005    &        0.037     \\\\\n",
       "\\textbf{x20}   &       0.0098  &        0.003     &     2.840  &         0.005        &        0.003    &        0.017     \\\\\n",
       "\\textbf{x21}   &      -0.0037  &        0.003     &    -1.083  &         0.279        &       -0.010    &        0.003     \\\\\n",
       "\\textbf{x22}   &      -0.0019  &        0.003     &    -0.555  &         0.579        &       -0.009    &        0.005     \\\\\n",
       "\\textbf{x23}   &       0.0139  &        0.004     &     3.682  &         0.000        &        0.006    &        0.021     \\\\\n",
       "\\textbf{x24}   &       0.0044  &        0.005     &     0.907  &         0.365        &       -0.005    &        0.014     \\\\\n",
       "\\textbf{x25}   &      -0.0092  &        0.004     &    -2.433  &         0.015        &       -0.017    &       -0.002     \\\\\n",
       "\\textbf{x26}   &      -0.0309  &        0.005     &    -6.074  &         0.000        &       -0.041    &       -0.021     \\\\\n",
       "\\textbf{x27}   &       0.0212  &        0.005     &     4.064  &         0.000        &        0.011    &        0.031     \\\\\n",
       "\\textbf{x28}   &       0.0157  &        0.005     &     3.305  &         0.001        &        0.006    &        0.025     \\\\\n",
       "\\textbf{x29}   &      -0.0015  &        0.005     &    -0.307  &         0.759        &       -0.011    &        0.008     \\\\\n",
       "\\textbf{x30}   &      -0.0067  &        0.005     &    -1.369  &         0.171        &       -0.016    &        0.003     \\\\\n",
       "\\textbf{x31}   &      -0.0086  &        0.005     &    -1.748  &         0.080        &       -0.018    &        0.001     \\\\\n",
       "\\textbf{x32}   &       0.0308  &        0.005     &     5.927  &         0.000        &        0.021    &        0.041     \\\\\n",
       "\\textbf{x33}   &       0.0230  &        0.005     &     4.658  &         0.000        &        0.013    &        0.033     \\\\\n",
       "\\textbf{x34}   &      -0.0214  &        0.005     &    -4.361  &         0.000        &       -0.031    &       -0.012     \\\\\n",
       "\\textbf{x35}   &       0.0215  &        0.005     &     4.378  &         0.000        &        0.012    &        0.031     \\\\\n",
       "\\textbf{x36}   &      -0.0062  &        0.005     &    -1.221  &         0.222        &       -0.016    &        0.004     \\\\\n",
       "\\textbf{x37}   &      -0.0161  &        0.005     &    -3.276  &         0.001        &       -0.026    &       -0.006     \\\\\n",
       "\\textbf{x38}   &      -0.0307  &        0.005     &    -6.114  &         0.000        &       -0.041    &       -0.021     \\\\\n",
       "\\textbf{x39}   &      -0.0090  &        0.005     &    -1.893  &         0.058        &       -0.018    &        0.000     \\\\\n",
       "\\textbf{x40}   &       0.0010  &        0.005     &     0.207  &         0.836        &       -0.008    &        0.010     \\\\\n",
       "\\textbf{x41}   &      -0.0427  &        0.004     &   -11.195  &         0.000        &       -0.050    &       -0.035     \\\\\n",
       "\\textbf{x42}   &      -0.0061  &        0.005     &    -1.189  &         0.235        &       -0.016    &        0.004     \\\\\n",
       "\\textbf{x43}   &      -0.0268  &        0.005     &    -5.175  &         0.000        &       -0.037    &       -0.017     \\\\\n",
       "\\textbf{x44}   &      -0.0030  &        0.005     &    -0.627  &         0.531        &       -0.012    &        0.006     \\\\\n",
       "\\textbf{x45}   &      -0.0166  &        0.005     &    -3.349  &         0.001        &       -0.026    &       -0.007     \\\\\n",
       "\\textbf{x46}   &      -0.0279  &        0.005     &    -5.681  &         0.000        &       -0.038    &       -0.018     \\\\\n",
       "\\textbf{x47}   &      -0.0215  &        0.005     &    -4.341  &         0.000        &       -0.031    &       -0.012     \\\\\n",
       "\\textbf{x48}   &      -0.0269  &        0.005     &    -5.175  &         0.000        &       -0.037    &       -0.017     \\\\\n",
       "\\textbf{x49}   &      -0.0037  &        0.005     &    -0.741  &         0.459        &       -0.014    &        0.006     \\\\\n",
       "\\textbf{x50}   &      -0.0310  &        0.005     &    -6.344  &         0.000        &       -0.041    &       -0.021     \\\\\n",
       "\\textbf{x51}   &       0.0106  &        0.005     &     2.185  &         0.029        &        0.001    &        0.020     \\\\\n",
       "\\textbf{x52}   &      -0.0128  &        0.005     &    -2.550  &         0.011        &       -0.023    &       -0.003     \\\\\n",
       "\\textbf{x53}   &       0.0018  &        0.005     &     0.363  &         0.716        &       -0.008    &        0.012     \\\\\n",
       "\\textbf{x54}   &       0.0007  &        0.005     &     0.146  &         0.884        &       -0.009    &        0.011     \\\\\n",
       "\\textbf{x55}   &      -0.0246  &        0.005     &    -5.113  &         0.000        &       -0.034    &       -0.015     \\\\\n",
       "\\textbf{x56}   &      -0.0038  &        0.003     &    -1.176  &         0.240        &       -0.010    &        0.003     \\\\\n",
       "\\textbf{x57}   &      -0.2512  &        0.003     &   -73.767  &         0.000        &       -0.258    &       -0.245     \\\\\n",
       "\\textbf{x58}   &      -0.3631  &        0.003     &  -105.698  &         0.000        &       -0.370    &       -0.356     \\\\\n",
       "\\textbf{x59}   &      -0.0055  &        0.003     &    -1.685  &         0.092        &       -0.012    &        0.001     \\\\\n",
       "\\textbf{x60}   &       0.0035  &        0.003     &     1.054  &         0.292        &       -0.003    &        0.010     \\\\\n",
       "\\textbf{x61}   &       0.0179  &        0.004     &     4.590  &         0.000        &        0.010    &        0.026     \\\\\n",
       "\\textbf{x62}   &       0.0026  &        0.003     &     0.791  &         0.429        &       -0.004    &        0.009     \\\\\n",
       "\\textbf{x63}   &       0.0042  &        0.003     &     1.289  &         0.197        &       -0.002    &        0.011     \\\\\n",
       "\\textbf{x64}   &       0.0074  &        0.003     &     2.243  &         0.025        &        0.001    &        0.014     \\\\\n",
       "\\textbf{x65}   &      -0.0039  &        0.003     &    -1.189  &         0.234        &       -0.010    &        0.003     \\\\\n",
       "\\textbf{x66}   &       0.0026  &        0.003     &     0.780  &         0.435        &       -0.004    &        0.009     \\\\\n",
       "\\textbf{x67}   &      -0.0035  &        0.003     &    -1.043  &         0.297        &       -0.010    &        0.003     \\\\\n",
       "\\textbf{x68}   &      -0.0077  &        0.004     &    -1.816  &         0.069        &       -0.016    &        0.001     \\\\\n",
       "\\textbf{x69}   &      -0.0111  &        0.005     &    -2.172  &         0.030        &       -0.021    &       -0.001     \\\\\n",
       "\\textbf{x70}   &      -0.0122  &        0.004     &    -3.214  &         0.001        &       -0.020    &       -0.005     \\\\\n",
       "\\textbf{x71}   &      -0.0227  &        0.005     &    -4.689  &         0.000        &       -0.032    &       -0.013     \\\\\n",
       "\\textbf{x72}   &      -0.0181  &        0.005     &    -3.578  &         0.000        &       -0.028    &       -0.008     \\\\\n",
       "\\textbf{x73}   &      -0.0197  &        0.004     &    -4.509  &         0.000        &       -0.028    &       -0.011     \\\\\n",
       "\\textbf{x74}   &      -0.0304  &        0.005     &    -5.595  &         0.000        &       -0.041    &       -0.020     \\\\\n",
       "\\textbf{x75}   &      -0.0177  &        0.004     &    -4.420  &         0.000        &       -0.025    &       -0.010     \\\\\n",
       "\\textbf{x76}   &      -0.0085  &        0.004     &    -2.420  &         0.016        &       -0.015    &       -0.002     \\\\\n",
       "\\textbf{x77}   &      -0.0034  &        0.005     &    -0.755  &         0.450        &       -0.012    &        0.005     \\\\\n",
       "\\textbf{x78}   &      -0.0185  &        0.005     &    -3.659  &         0.000        &       -0.028    &       -0.009     \\\\\n",
       "\\textbf{x79}   &       0.0002  &        0.003     &     0.055  &         0.956        &       -0.007    &        0.007     \\\\\n",
       "\\textbf{x80}   &      -0.0031  &        0.003     &    -0.900  &         0.368        &       -0.010    &        0.004     \\\\\n",
       "\\textbf{x81}   &      -0.0095  &        0.004     &    -2.515  &         0.012        &       -0.017    &       -0.002     \\\\\n",
       "\\textbf{x82}   &      -0.0142  &        0.004     &    -3.241  &         0.001        &       -0.023    &       -0.006     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 12172.876 & \\textbf{  Durbin-Watson:     } &     2.003  \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 69343.759  \\\\\n",
       "\\textbf{Skew:}          &    0.528  & \\textbf{  Prob(JB):          } &      0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &    7.179  & \\textbf{  Cond. No.          } &  4.57e+15  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 2.85e-26. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:           DurationSecs   R-squared:                       0.187\n",
       "Model:                            OLS   Adj. R-squared:                  0.186\n",
       "Method:                 Least Squares   F-statistic:                     254.5\n",
       "Date:                Tue, 05 Nov 2024   Prob (F-statistic):               0.00\n",
       "Time:                        19:19:04   Log-Likelihood:            -1.2498e+05\n",
       "No. Observations:               89592   AIC:                         2.501e+05\n",
       "Df Residuals:                   89510   BIC:                         2.509e+05\n",
       "Df Model:                          81                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.5155      0.003   1077.245      0.000       3.509       3.522\n",
       "x1             0.0171      0.033      0.515      0.606      -0.048       0.082\n",
       "x2             0.1386      0.024      5.888      0.000       0.092       0.185\n",
       "x3            -0.0502      0.004    -11.285      0.000      -0.059      -0.041\n",
       "x4             0.1960      0.024      8.310      0.000       0.150       0.242\n",
       "x5             0.0880      0.006     15.198      0.000       0.077       0.099\n",
       "x6             0.0346      0.003     10.131      0.000       0.028       0.041\n",
       "x7            -0.1370      0.005    -28.244      0.000      -0.147      -0.127\n",
       "x8            -0.0148      0.004     -4.149      0.000      -0.022      -0.008\n",
       "x9             0.0171      0.004      4.058      0.000       0.009       0.025\n",
       "x10            0.0351      0.010      3.369      0.001       0.015       0.056\n",
       "x11            0.0629      0.004     14.713      0.000       0.055       0.071\n",
       "x12           -0.4380      0.060     -7.350      0.000      -0.555      -0.321\n",
       "x13           -0.0239      0.030     -0.798      0.425      -0.083       0.035\n",
       "x14            0.1704      0.021      8.285      0.000       0.130       0.211\n",
       "x15            0.0696      0.006     11.062      0.000       0.057       0.082\n",
       "x16            0.0158      0.032      0.500      0.617      -0.046       0.078\n",
       "x17            0.0258      0.027      0.970      0.332      -0.026       0.078\n",
       "x18           -0.0191      0.003     -5.674      0.000      -0.026      -0.013\n",
       "x19            0.0211      0.008      2.612      0.009       0.005       0.037\n",
       "x20            0.0098      0.003      2.840      0.005       0.003       0.017\n",
       "x21           -0.0037      0.003     -1.083      0.279      -0.010       0.003\n",
       "x22           -0.0019      0.003     -0.555      0.579      -0.009       0.005\n",
       "x23            0.0139      0.004      3.682      0.000       0.006       0.021\n",
       "x24            0.0044      0.005      0.907      0.365      -0.005       0.014\n",
       "x25           -0.0092      0.004     -2.433      0.015      -0.017      -0.002\n",
       "x26           -0.0309      0.005     -6.074      0.000      -0.041      -0.021\n",
       "x27            0.0212      0.005      4.064      0.000       0.011       0.031\n",
       "x28            0.0157      0.005      3.305      0.001       0.006       0.025\n",
       "x29           -0.0015      0.005     -0.307      0.759      -0.011       0.008\n",
       "x30           -0.0067      0.005     -1.369      0.171      -0.016       0.003\n",
       "x31           -0.0086      0.005     -1.748      0.080      -0.018       0.001\n",
       "x32            0.0308      0.005      5.927      0.000       0.021       0.041\n",
       "x33            0.0230      0.005      4.658      0.000       0.013       0.033\n",
       "x34           -0.0214      0.005     -4.361      0.000      -0.031      -0.012\n",
       "x35            0.0215      0.005      4.378      0.000       0.012       0.031\n",
       "x36           -0.0062      0.005     -1.221      0.222      -0.016       0.004\n",
       "x37           -0.0161      0.005     -3.276      0.001      -0.026      -0.006\n",
       "x38           -0.0307      0.005     -6.114      0.000      -0.041      -0.021\n",
       "x39           -0.0090      0.005     -1.893      0.058      -0.018       0.000\n",
       "x40            0.0010      0.005      0.207      0.836      -0.008       0.010\n",
       "x41           -0.0427      0.004    -11.195      0.000      -0.050      -0.035\n",
       "x42           -0.0061      0.005     -1.189      0.235      -0.016       0.004\n",
       "x43           -0.0268      0.005     -5.175      0.000      -0.037      -0.017\n",
       "x44           -0.0030      0.005     -0.627      0.531      -0.012       0.006\n",
       "x45           -0.0166      0.005     -3.349      0.001      -0.026      -0.007\n",
       "x46           -0.0279      0.005     -5.681      0.000      -0.038      -0.018\n",
       "x47           -0.0215      0.005     -4.341      0.000      -0.031      -0.012\n",
       "x48           -0.0269      0.005     -5.175      0.000      -0.037      -0.017\n",
       "x49           -0.0037      0.005     -0.741      0.459      -0.014       0.006\n",
       "x50           -0.0310      0.005     -6.344      0.000      -0.041      -0.021\n",
       "x51            0.0106      0.005      2.185      0.029       0.001       0.020\n",
       "x52           -0.0128      0.005     -2.550      0.011      -0.023      -0.003\n",
       "x53            0.0018      0.005      0.363      0.716      -0.008       0.012\n",
       "x54            0.0007      0.005      0.146      0.884      -0.009       0.011\n",
       "x55           -0.0246      0.005     -5.113      0.000      -0.034      -0.015\n",
       "x56           -0.0038      0.003     -1.176      0.240      -0.010       0.003\n",
       "x57           -0.2512      0.003    -73.767      0.000      -0.258      -0.245\n",
       "x58           -0.3631      0.003   -105.698      0.000      -0.370      -0.356\n",
       "x59           -0.0055      0.003     -1.685      0.092      -0.012       0.001\n",
       "x60            0.0035      0.003      1.054      0.292      -0.003       0.010\n",
       "x61            0.0179      0.004      4.590      0.000       0.010       0.026\n",
       "x62            0.0026      0.003      0.791      0.429      -0.004       0.009\n",
       "x63            0.0042      0.003      1.289      0.197      -0.002       0.011\n",
       "x64            0.0074      0.003      2.243      0.025       0.001       0.014\n",
       "x65           -0.0039      0.003     -1.189      0.234      -0.010       0.003\n",
       "x66            0.0026      0.003      0.780      0.435      -0.004       0.009\n",
       "x67           -0.0035      0.003     -1.043      0.297      -0.010       0.003\n",
       "x68           -0.0077      0.004     -1.816      0.069      -0.016       0.001\n",
       "x69           -0.0111      0.005     -2.172      0.030      -0.021      -0.001\n",
       "x70           -0.0122      0.004     -3.214      0.001      -0.020      -0.005\n",
       "x71           -0.0227      0.005     -4.689      0.000      -0.032      -0.013\n",
       "x72           -0.0181      0.005     -3.578      0.000      -0.028      -0.008\n",
       "x73           -0.0197      0.004     -4.509      0.000      -0.028      -0.011\n",
       "x74           -0.0304      0.005     -5.595      0.000      -0.041      -0.020\n",
       "x75           -0.0177      0.004     -4.420      0.000      -0.025      -0.010\n",
       "x76           -0.0085      0.004     -2.420      0.016      -0.015      -0.002\n",
       "x77           -0.0034      0.005     -0.755      0.450      -0.012       0.005\n",
       "x78           -0.0185      0.005     -3.659      0.000      -0.028      -0.009\n",
       "x79            0.0002      0.003      0.055      0.956      -0.007       0.007\n",
       "x80           -0.0031      0.003     -0.900      0.368      -0.010       0.004\n",
       "x81           -0.0095      0.004     -2.515      0.012      -0.017      -0.002\n",
       "x82           -0.0142      0.004     -3.241      0.001      -0.023      -0.006\n",
       "==============================================================================\n",
       "Omnibus:                    12172.876   Durbin-Watson:                   2.003\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            69343.759\n",
       "Skew:                           0.528   Prob(JB):                         0.00\n",
       "Kurtosis:                       7.179   Cond. No.                     4.57e+15\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.85e-26. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_scaled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0ad0b301-73ef-4d5f-a33f-c08f76ed508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e02f7769-2014-49b2-bdcb-97804ad4199f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "55075098-c641-4343-bc98-44e36712962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "num_features=X_train.shape[1]\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "fad99ef9-faa1-4640-be72-04711be83e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Input(shape=(num_features,))) # the input layer where the shape of inputs is specified, for now it is just the number of features (more advanced inputs like image data would have more dimension)\n",
    "model.add(Dense(20, activation='relu')) \n",
    "model.add(Dense(20, kernel_regularizer=regularizers.l2(0.001), activation='relu')) \n",
    "model.add(Dense(1, activation='linear')) # the output layer has 1 unit, the linear activation is used as this is for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "db67c7e2-809e-4651-94ae-b4c20ec0ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='MSE', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "005a3624-52c5-45a1-a384-5fea39c8d663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,660</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m1,660\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,101</span> (8.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,101\u001b[0m (8.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,101</span> (8.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,101\u001b[0m (8.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "35c61be9-2ba2-4e42-b8f3-f42f837203ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 994us/step - loss: 1.5328 - val_loss: 0.9745\n",
      "Epoch 2/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 956us/step - loss: 0.9583 - val_loss: 0.9543\n",
      "Epoch 3/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 973us/step - loss: 0.9399 - val_loss: 0.9480\n",
      "Epoch 4/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 975us/step - loss: 0.9299 - val_loss: 0.9438\n",
      "Epoch 5/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 971us/step - loss: 0.9230 - val_loss: 0.9410\n",
      "Epoch 6/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 971us/step - loss: 0.9182 - val_loss: 0.9389\n",
      "Epoch 7/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 965us/step - loss: 0.9145 - val_loss: 0.9372\n",
      "Epoch 8/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.9111 - val_loss: 0.9358\n",
      "Epoch 9/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.9085 - val_loss: 0.9351\n",
      "Epoch 10/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 973us/step - loss: 0.9060 - val_loss: 0.9348\n",
      "Epoch 11/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 997us/step - loss: 0.9039 - val_loss: 0.9344\n",
      "Epoch 12/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.9023 - val_loss: 0.9339\n",
      "Epoch 13/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.9010 - val_loss: 0.9331\n",
      "Epoch 14/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 963us/step - loss: 0.8997 - val_loss: 0.9335\n",
      "Epoch 15/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8987 - val_loss: 0.9331\n",
      "Epoch 16/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 996us/step - loss: 0.8977 - val_loss: 0.9332\n",
      "Epoch 17/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 997us/step - loss: 0.8968 - val_loss: 0.9326\n",
      "Epoch 18/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8958 - val_loss: 0.9324\n",
      "Epoch 19/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8946 - val_loss: 0.9322\n",
      "Epoch 20/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.8938 - val_loss: 0.9317\n",
      "Epoch 21/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 974us/step - loss: 0.8934 - val_loss: 0.9313\n",
      "Epoch 22/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 958us/step - loss: 0.8929 - val_loss: 0.9312\n",
      "Epoch 23/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 941us/step - loss: 0.8924 - val_loss: 0.9306\n",
      "Epoch 24/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8917 - val_loss: 0.9310\n",
      "Epoch 25/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 998us/step - loss: 0.8911 - val_loss: 0.9305\n",
      "Epoch 26/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8906 - val_loss: 0.9306\n",
      "Epoch 27/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8905 - val_loss: 0.9308\n",
      "Epoch 28/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 964us/step - loss: 0.8903 - val_loss: 0.9312\n",
      "Epoch 29/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8897 - val_loss: 0.9310\n",
      "Epoch 30/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8895 - val_loss: 0.9308\n",
      "Epoch 31/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 959us/step - loss: 0.8890 - val_loss: 0.9304\n",
      "Epoch 32/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 963us/step - loss: 0.8887 - val_loss: 0.9309\n",
      "Epoch 33/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 956us/step - loss: 0.8883 - val_loss: 0.9309\n",
      "Epoch 34/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 939us/step - loss: 0.8881 - val_loss: 0.9307\n",
      "Epoch 35/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 945us/step - loss: 0.8879 - val_loss: 0.9306\n",
      "Epoch 36/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 949us/step - loss: 0.8875 - val_loss: 0.9307\n",
      "Epoch 37/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 958us/step - loss: 0.8873 - val_loss: 0.9305\n",
      "Epoch 38/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 979us/step - loss: 0.8871 - val_loss: 0.9303\n",
      "Epoch 39/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 973us/step - loss: 0.8868 - val_loss: 0.9302\n",
      "Epoch 40/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 962us/step - loss: 0.8865 - val_loss: 0.9299\n",
      "Epoch 41/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 941us/step - loss: 0.8864 - val_loss: 0.9298\n",
      "Epoch 42/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 980us/step - loss: 0.8860 - val_loss: 0.9300\n",
      "Epoch 43/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 937us/step - loss: 0.8860 - val_loss: 0.9301\n",
      "Epoch 44/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 971us/step - loss: 0.8858 - val_loss: 0.9300\n",
      "Epoch 45/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 935us/step - loss: 0.8856 - val_loss: 0.9302\n",
      "Epoch 46/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8853 - val_loss: 0.9304\n",
      "Epoch 47/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 970us/step - loss: 0.8851 - val_loss: 0.9303\n",
      "Epoch 48/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 962us/step - loss: 0.8849 - val_loss: 0.9301\n",
      "Epoch 49/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 969us/step - loss: 0.8849 - val_loss: 0.9300\n",
      "Epoch 50/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 943us/step - loss: 0.8847 - val_loss: 0.9303\n",
      "Epoch 51/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 963us/step - loss: 0.8845 - val_loss: 0.9299\n",
      "Epoch 52/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 946us/step - loss: 0.8843 - val_loss: 0.9298\n",
      "Epoch 53/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 967us/step - loss: 0.8842 - val_loss: 0.9297\n",
      "Epoch 54/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 959us/step - loss: 0.8842 - val_loss: 0.9299\n",
      "Epoch 55/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8839 - val_loss: 0.9298\n",
      "Epoch 56/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 990us/step - loss: 0.8839 - val_loss: 0.9299\n",
      "Epoch 57/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8837 - val_loss: 0.9297\n",
      "Epoch 58/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8833 - val_loss: 0.9299\n",
      "Epoch 59/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8832 - val_loss: 0.9300\n",
      "Epoch 60/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8828 - val_loss: 0.9304\n",
      "Epoch 61/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8830 - val_loss: 0.9301\n",
      "Epoch 62/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8827 - val_loss: 0.9301\n",
      "Epoch 63/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 996us/step - loss: 0.8827 - val_loss: 0.9301\n",
      "Epoch 64/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8824 - val_loss: 0.9301\n",
      "Epoch 65/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8823 - val_loss: 0.9304\n",
      "Epoch 66/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8822 - val_loss: 0.9303\n",
      "Epoch 67/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8820 - val_loss: 0.9304\n",
      "Epoch 68/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8820 - val_loss: 0.9304\n",
      "Epoch 69/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 0.8819 - val_loss: 0.9301\n",
      "Epoch 70/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8821 - val_loss: 0.9301\n",
      "Epoch 71/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8819 - val_loss: 0.9302\n",
      "Epoch 72/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8819 - val_loss: 0.9300\n",
      "Epoch 73/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8817 - val_loss: 0.9297\n",
      "Epoch 74/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8817 - val_loss: 0.9302\n",
      "Epoch 75/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 998us/step - loss: 0.8816 - val_loss: 0.9298\n",
      "Epoch 76/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8813 - val_loss: 0.9295\n",
      "Epoch 77/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8813 - val_loss: 0.9296\n",
      "Epoch 78/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8812 - val_loss: 0.9300\n",
      "Epoch 79/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8811 - val_loss: 0.9302\n",
      "Epoch 80/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8810 - val_loss: 0.9302\n",
      "Epoch 81/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8810 - val_loss: 0.9304\n",
      "Epoch 82/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 977us/step - loss: 0.8810 - val_loss: 0.9304\n",
      "Epoch 83/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8808 - val_loss: 0.9306\n",
      "Epoch 84/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 993us/step - loss: 0.8808 - val_loss: 0.9307\n",
      "Epoch 85/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8807 - val_loss: 0.9307\n",
      "Epoch 86/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8805 - val_loss: 0.9305\n",
      "Epoch 87/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 986us/step - loss: 0.8806 - val_loss: 0.9306\n",
      "Epoch 88/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8805 - val_loss: 0.9309\n",
      "Epoch 89/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8806 - val_loss: 0.9305\n",
      "Epoch 90/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 999us/step - loss: 0.8804 - val_loss: 0.9308\n",
      "Epoch 91/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 986us/step - loss: 0.8803 - val_loss: 0.9308\n",
      "Epoch 92/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 995us/step - loss: 0.8803 - val_loss: 0.9310\n",
      "Epoch 93/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 999us/step - loss: 0.8803 - val_loss: 0.9308\n",
      "Epoch 94/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 977us/step - loss: 0.8801 - val_loss: 0.9311\n",
      "Epoch 95/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8801 - val_loss: 0.9309\n",
      "Epoch 96/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 966us/step - loss: 0.8800 - val_loss: 0.9306\n",
      "Epoch 97/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8800 - val_loss: 0.9306\n",
      "Epoch 98/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 974us/step - loss: 0.8798 - val_loss: 0.9306\n",
      "Epoch 99/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8800 - val_loss: 0.9311\n",
      "Epoch 100/100\n",
      "\u001b[1m8960/8960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 0.8799 - val_loss: 0.9304\n"
     ]
    }
   ],
   "source": [
    "modelout = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6d77e3b4-6ca0-4d2a-b0af-0b7776b2c974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(modelout.history['loss'], label='Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "97c39c9c-e665-4a61-85c1-77c6418ac0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE on the test data for neural net:  0.9304479956626892\n",
      "MSE on the test data for linear reg:  0.954762686332319\n"
     ]
    }
   ],
   "source": [
    "MSE_nn = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('\\nMSE on the test data for neural net: ', MSE_nn)\n",
    "print('MSE on the test data for linear reg: ', MSE_lm_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
